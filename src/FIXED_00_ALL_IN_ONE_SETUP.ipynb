{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f661a3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ALL-IN-ONE SETUP NOTEBOOK - FIXED VERSION\n",
    "# iFood 2019 AlexNet Project - Complete Automation\n",
    "# \n",
    "# Repository: https://github.com/deftorch/alexnet-ifood2019\n",
    "# Author: deftorch (qaidhaidaradila@gmail.com)\n",
    "#\n",
    "# FIXES:\n",
    "# ‚úÖ Complete image download added\n",
    "# ‚úÖ Repository structure verification\n",
    "# ‚úÖ Better error handling\n",
    "# ‚úÖ GPU checks improved\n",
    "# ============================================================\n",
    "\n",
    "# ============================================================\n",
    "# PART 0: SANITY CHECK\n",
    "# ============================================================\n",
    "print(\"=\"*70)\n",
    "print(\"üîç Pre-flight checks...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "import sys\n",
    "\n",
    "# Check Python version\n",
    "python_version = sys.version_info\n",
    "if python_version.major < 3 or (python_version.major == 3 and python_version.minor < 8):\n",
    "    raise Exception(f\"‚ùå Python 3.8+ required. Current: {sys.version}\")\n",
    "\n",
    "print(f\"‚úÖ Python {python_version.major}.{python_version.minor}.{python_version.micro}\")\n",
    "\n",
    "# Check if running in Colab\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    print(\"‚úÖ Running in Google Colab\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  Not in Colab - some features may not work\")\n",
    "    raise Exception(\"This notebook requires Google Colab\")\n",
    "\n",
    "print(\"\\n‚úÖ Pre-flight checks passed!\\n\")\n",
    "\n",
    "# ============================================================\n",
    "# PART 1: MOUNT GOOGLE DRIVE & CREATE STRUCTURE\n",
    "# ============================================================\n",
    "print(\"=\"*70)\n",
    "print(\"üöÄ iFood 2019 AlexNet Project - Automated Setup\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nPART 1: Setting up Google Drive...\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Mount Drive\n",
    "print(f\"\\nüìÇ Mounting Google Drive...\")\n",
    "drive.mount('/content/drive', force_remount=False)\n",
    "\n",
    "# Define paths\n",
    "DRIVE_ROOT = '/content/drive/MyDrive'\n",
    "PROJECT_NAME = 'AlexNet_iFood2019'\n",
    "PROJECT_PATH = os.path.join(DRIVE_ROOT, PROJECT_NAME)\n",
    "\n",
    "# Create folder structure\n",
    "folders = {\n",
    "    'dataset': 'Dataset iFood 2019',\n",
    "    'checkpoints': 'Model checkpoints',\n",
    "    'evaluation_results': 'Evaluation metrics',\n",
    "    'analysis_results': 'Analysis plots',\n",
    "    'logs': 'Training logs'\n",
    "}\n",
    "\n",
    "print(f\"\\nüìÅ Creating project structure at:\\n   {PROJECT_PATH}\\n\")\n",
    "for folder, description in folders.items():\n",
    "    folder_path = os.path.join(PROJECT_PATH, folder)\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    print(f\"  ‚úì {folder:20s} - {description}\")\n",
    "\n",
    "print(\"\\n‚úÖ Google Drive setup complete!\")\n",
    "\n",
    "# ============================================================\n",
    "# PART 2: INSTALL ALL DEPENDENCIES\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PART 2: Installing dependencies...\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "!pip install --upgrade pip -q\n",
    "!pip install -q torch torchvision torchaudio\n",
    "!pip install -q pandas numpy pillow opencv-python scikit-learn\n",
    "!pip install -q matplotlib seaborn tqdm\n",
    "!pip install -q gdown requests\n",
    "\n",
    "print(\"\\n‚úÖ All dependencies installed!\")\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(f\"\\nüìä Installed versions:\")\n",
    "print(f\"  PyTorch: {torch.__version__}\")\n",
    "print(f\"  Pandas: {pd.__version__}\")\n",
    "print(f\"  NumPy: {np.__version__}\")\n",
    "print(f\"  CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# ============================================================\n",
    "# PART 3: DOWNLOAD DATASET WITH IMAGES\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PART 3: Dataset Download (WITH IMAGES)\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "import requests\n",
    "import tarfile\n",
    "from tqdm import tqdm\n",
    "\n",
    "DATASET_DIR = os.path.join(PROJECT_PATH, 'dataset')\n",
    "\n",
    "def download_file(url, output_path):\n",
    "    \"\"\"Download file with progress bar\"\"\"\n",
    "    print(f\"  üì• Downloading from {url}\")\n",
    "    response = requests.get(url, stream=True, timeout=300)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    total_size = int(response.headers.get('content-length', 0))\n",
    "    \n",
    "    with open(output_path, 'wb') as f, tqdm(\n",
    "        total=total_size,\n",
    "        unit='iB',\n",
    "        unit_scale=True,\n",
    "        unit_divisor=1024,\n",
    "    ) as pbar:\n",
    "        for data in response.iter_content(chunk_size=1024*1024):\n",
    "            f.write(data)\n",
    "            pbar.update(len(data))\n",
    "\n",
    "def extract_tar(tar_path, extract_to):\n",
    "    \"\"\"Extract tar file\"\"\"\n",
    "    print(f\"  üì¶ Extracting {os.path.basename(tar_path)}...\")\n",
    "    with tarfile.open(tar_path, 'r:*') as tar:\n",
    "        tar.extractall(path=extract_to)\n",
    "\n",
    "# Check if images already exist\n",
    "def count_images(directory):\n",
    "    if os.path.exists(directory) and os.path.isdir(directory):\n",
    "        return len([f for f in os.listdir(directory) \n",
    "                   if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "    return 0\n",
    "\n",
    "train_count = count_images(os.path.join(DATASET_DIR, 'train_images'))\n",
    "\n",
    "if train_count < 100000:  # Threshold for \"complete\" dataset\n",
    "    print(\"\\n‚ö†Ô∏è  Images missing or incomplete!\")\n",
    "    print(\"üì¶ Downloading iFood 2019 dataset (~3.1 GB)\\n\")\n",
    "    \n",
    "    # Dataset URLs\n",
    "    DATASETS = {\n",
    "        'train': {\n",
    "            'url': 'https://food-x.s3.amazonaws.com/train.tar',\n",
    "            'size': '2.3 GB'\n",
    "        },\n",
    "        'val': {\n",
    "            'url': 'https://food-x.s3.amazonaws.com/val.tar',\n",
    "            'size': '231 MB'\n",
    "        },\n",
    "        'test': {\n",
    "            'url': 'https://food-x.s3.amazonaws.com/test.tar',\n",
    "            'size': '548 MB'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    DOWNLOAD_DIR = '/content/ifood_downloads'\n",
    "    os.makedirs(DOWNLOAD_DIR, exist_ok=True)\n",
    "    \n",
    "    # Download and extract each split\n",
    "    for i, (name, info) in enumerate(DATASETS.items(), 1):\n",
    "        print(f\"\\n[{i}/3] {name.upper()} SET ({info['size']})\")\n",
    "        print(\"-\"*70)\n",
    "        \n",
    "        tar_path = os.path.join(DOWNLOAD_DIR, f\"{name}.tar\")\n",
    "        \n",
    "        # Download\n",
    "        try:\n",
    "            download_file(info['url'], tar_path)\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Download failed: {e}\")\n",
    "            print(f\"  üí° Try downloading manually from: {info['url']}\")\n",
    "            continue\n",
    "        \n",
    "        # Extract\n",
    "        try:\n",
    "            extract_tar(tar_path, DATASET_DIR)\n",
    "            \n",
    "            # Rename to standard names\n",
    "            old_path = os.path.join(DATASET_DIR, name)\n",
    "            new_path = os.path.join(DATASET_DIR, f'{name}_images')\n",
    "            \n",
    "            if os.path.exists(old_path) and not os.path.exists(new_path):\n",
    "                os.rename(old_path, new_path)\n",
    "            \n",
    "            print(f\"  ‚úÖ {name} complete!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Extraction failed: {e}\")\n",
    "            continue\n",
    "        finally:\n",
    "            # Cleanup\n",
    "            if os.path.exists(tar_path):\n",
    "                os.remove(tar_path)\n",
    "    \n",
    "    print(\"\\n‚úÖ Dataset download complete!\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Images already exist ({train_count:,} training images)\")\n",
    "    print(\"‚è© Skipping download\")\n",
    "\n",
    "# ============================================================\n",
    "# PART 4: VERIFY DATASET\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PART 4: Verifying dataset\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "required = {\n",
    "    'files': ['train_info.csv', 'val_info.csv', 'test_info.csv', 'class_list.txt'],\n",
    "    'dirs': ['train_images', 'val_images', 'test_images']\n",
    "}\n",
    "\n",
    "print(\"\\nüìÑ Checking files:\")\n",
    "for file in required['files']:\n",
    "    path = os.path.join(DATASET_DIR, file)\n",
    "    exists = os.path.exists(path)\n",
    "    print(f\"  {'‚úÖ' if exists else '‚ùå'} {file}\")\n",
    "\n",
    "print(\"\\nüìÅ Checking image directories:\")\n",
    "total_images = 0\n",
    "for dir_name in required['dirs']:\n",
    "    path = os.path.join(DATASET_DIR, dir_name)\n",
    "    count = count_images(path)\n",
    "    total_images += count\n",
    "    print(f\"  {'‚úÖ' if count > 0 else '‚ùå'} {dir_name:15s} - {count:,} images\")\n",
    "\n",
    "if total_images > 0:\n",
    "    print(f\"\\n‚úÖ Dataset verified! Total images: {total_images:,}\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå No images found!\")\n",
    "    print(f\"üí° Check dataset manually or re-run PART 3\")\n",
    "\n",
    "# ============================================================\n",
    "# PART 5: CLONE REPOSITORY\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PART 5: Cloning repository\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "REPO_URL = \"https://github.com/deftorch/alexnet-ifood2019.git\"\n",
    "REPO_DIR = \"/content/alexnet-ifood2019\"\n",
    "\n",
    "if os.path.exists(REPO_DIR):\n",
    "    print(\"\\n‚è≥ Removing old repository...\")\n",
    "    !rm -rf {REPO_DIR}\n",
    "\n",
    "print(f\"\\nüì• Cloning from: {REPO_URL}\")\n",
    "!git clone -q {REPO_URL} {REPO_DIR}\n",
    "\n",
    "os.chdir(REPO_DIR)\n",
    "print(f\"‚úÖ Repository cloned!\")\n",
    "print(f\"üìÇ Working directory: {os.getcwd()}\")\n",
    "\n",
    "# Verify repository structure\n",
    "print(\"\\nüìÅ Verifying repository structure:\")\n",
    "required_paths = [\n",
    "    'src',\n",
    "    'src/models',\n",
    "    'src/models/alexnet.py',\n",
    "    'src/data_loader.py',\n",
    "    'src/train.py',\n",
    "    'src/evaluate.py'\n",
    "]\n",
    "\n",
    "all_exist = True\n",
    "for path in required_paths:\n",
    "    full_path = os.path.join(REPO_DIR, path)\n",
    "    exists = os.path.exists(full_path)\n",
    "    print(f\"  {'‚úÖ' if exists else '‚ùå'} {path}\")\n",
    "    if not exists:\n",
    "        all_exist = False\n",
    "\n",
    "if not all_exist:\n",
    "    print(\"\\n‚ùå Repository structure incomplete!\")\n",
    "    print(\"üí° Make sure all source files are pushed to GitHub\")\n",
    "    print(\"üí° Or use the generated source code files\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ Repository structure verified!\")\n",
    "\n",
    "# ============================================================\n",
    "# PART 6: CREATE SYMBOLIC LINKS\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PART 6: Creating symbolic links\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "LINKS = {\n",
    "    'data': 'dataset',\n",
    "    'checkpoints': 'checkpoints',\n",
    "    'evaluation_results': 'evaluation_results',\n",
    "    'analysis_results': 'analysis_results',\n",
    "    'logs': 'logs'\n",
    "}\n",
    "\n",
    "print(\"\\nüîó Creating links...\\n\")\n",
    "\n",
    "for local_name, drive_folder in LINKS.items():\n",
    "    local_path = os.path.join(REPO_DIR, local_name)\n",
    "    drive_path = os.path.join(PROJECT_PATH, drive_folder)\n",
    "    \n",
    "    if os.path.exists(local_path):\n",
    "        !rm -rf {local_path}\n",
    "    \n",
    "    !ln -s {drive_path} {local_path}\n",
    "    \n",
    "    if os.path.islink(local_path):\n",
    "        print(f\"  ‚úÖ {local_name:20s} -> {drive_folder}\")\n",
    "    else:\n",
    "        print(f\"  ‚ùå {local_name:20s} - Failed\")\n",
    "\n",
    "print(\"\\n‚úÖ All links created!\")\n",
    "\n",
    "# ============================================================\n",
    "# PART 7: SETUP PYTHON ENVIRONMENT\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PART 7: Configuring Python environment\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "paths_to_add = [\n",
    "    REPO_DIR,\n",
    "    os.path.join(REPO_DIR, 'src'),\n",
    "]\n",
    "\n",
    "for path in paths_to_add:\n",
    "    if path not in sys.path:\n",
    "        sys.path.insert(0, path)\n",
    "        print(f\"‚úÖ Added to path: {path}\")\n",
    "\n",
    "# ============================================================\n",
    "# PART 8: TEST IMPORTS\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PART 8: Testing imports\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "try:\n",
    "    from src.models.alexnet import get_model\n",
    "    from src.data_loader import get_dataloaders\n",
    "    \n",
    "    print(\"\\n‚úÖ All imports successful!\")\n",
    "    \n",
    "    # Test model creation\n",
    "    print(\"\\nüß™ Testing models:\")\n",
    "    for model_name in ['alexnet_baseline', 'alexnet_mod1', \n",
    "                       'alexnet_mod2', 'alexnet_combined']:\n",
    "        model = get_model(model_name, num_classes=251)\n",
    "        params = sum(p.numel() for p in model.parameters())\n",
    "        print(f\"  ‚úÖ {model_name:20s} - {params:,} parameters\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Import error: {e}\")\n",
    "    print(\"\\nüí° Make sure:\")\n",
    "    print(\"  1. Repository has complete src/ folder\")\n",
    "    print(\"  2. All Python files are present\")\n",
    "    print(\"  3. __init__.py files exist\")\n",
    "\n",
    "# ============================================================\n",
    "# PART 9: GPU CONFIGURATION\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PART 9: GPU configuration\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    \n",
    "    print(f\"\\nüéÆ GPU Information:\")\n",
    "    print(f\"  Device: {gpu_name}\")\n",
    "    print(f\"  Memory: {gpu_mem:.2f} GB\")\n",
    "    \n",
    "    # Recommended batch size\n",
    "    if gpu_mem >= 15:\n",
    "        batch_size = 128\n",
    "        print(f\"  Recommended batch size: 128-256\")\n",
    "    elif gpu_mem >= 12:\n",
    "        batch_size = 64\n",
    "        print(f\"  Recommended batch size: 64-128\")\n",
    "    else:\n",
    "        batch_size = 32\n",
    "        print(f\"  Recommended batch size: 32-64\")\n",
    "    \n",
    "    print(\"\\n‚úÖ GPU ready for training!\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  WARNING: GPU NOT AVAILABLE!\")\n",
    "    print(\"\\nüí° To enable GPU:\")\n",
    "    print(\"  1. Runtime > Change runtime type\")\n",
    "    print(\"  2. Hardware accelerator > GPU (T4)\")\n",
    "    print(\"  3. Save and reconnect\")\n",
    "    print(\"\\n‚ö†Ô∏è  Training on CPU will be EXTREMELY slow!\")\n",
    "    batch_size = 32\n",
    "\n",
    "# ============================================================\n",
    "# FINAL SUMMARY\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚ú® SETUP COMPLETE ‚ú®\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\"\"\n",
    "‚úÖ All components ready!\n",
    "\n",
    "üìÇ Project: {PROJECT_PATH}\n",
    "üì¶ Repository: {REPO_DIR}\n",
    "üìä Dataset: {total_images:,} images\n",
    "üéÆ GPU: {\"‚úÖ \" + (gpu_name if torch.cuda.is_available() else \"‚ùå Not available\")}\n",
    "\n",
    "üöÄ Start Training:\n",
    "  python src/train.py \\\\\n",
    "      --data_dir data \\\\\n",
    "      --model_name alexnet_baseline \\\\\n",
    "      --num_epochs 50 \\\\\n",
    "      --batch_size {batch_size} \\\\\n",
    "      --lr 0.01\n",
    "\n",
    "üìä Evaluate Model:\n",
    "  python src/evaluate.py \\\\\n",
    "      --model_name alexnet_baseline \\\\\n",
    "      --checkpoint checkpoints/alexnet_baseline/best_model.pth\n",
    "\n",
    "üí° All data auto-saves to Google Drive!\n",
    "\n",
    "{\"=\"*70}\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
