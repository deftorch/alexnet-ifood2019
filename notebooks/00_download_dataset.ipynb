{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üì• Download Dataset iFood 2019 - Otomatis\n",
                "\n",
                "Notebook ini akan **otomatis mendownload dan mengekstrak** dataset iFood 2019 ke Google Drive.\n",
                "\n",
                "### Dataset Info:\n",
                "| File | Size | Isi |\n",
                "|------|------|-----|\n",
                "| Annotations | 3 MB | Labels & class list |\n",
                "| Train Images | 2.3 GB | 120,216 gambar |\n",
                "| Val Images | 231 MB | 12,170 gambar |\n",
                "| Test Images | 548 MB | 28,399 gambar |\n",
                "\n",
                "**Total: ~3.1 GB**\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# STEP 1: Mount Google Drive\n",
                "# ============================================================\n",
                "\n",
                "from google.colab import drive\n",
                "drive.mount('/content/drive')\n",
                "\n",
                "import os\n",
                "\n",
                "# Buat folder struktur\n",
                "PROJECT_PATH = '/content/drive/MyDrive/AlexNet_iFood2019'\n",
                "DATASET_PATH = os.path.join(PROJECT_PATH, 'dataset')\n",
                "\n",
                "os.makedirs(DATASET_PATH, exist_ok=True)\n",
                "os.makedirs(os.path.join(PROJECT_PATH, 'checkpoints'), exist_ok=True)\n",
                "os.makedirs(os.path.join(PROJECT_PATH, 'evaluation_results'), exist_ok=True)\n",
                "os.makedirs(os.path.join(PROJECT_PATH, 'analysis_results'), exist_ok=True)\n",
                "\n",
                "print(f\"‚úÖ Google Drive mounted\")\n",
                "print(f\"üìÅ Dataset akan disimpan di: {DATASET_PATH}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# STEP 2: Download Dataset\n",
                "# ============================================================\n",
                "\n",
                "import os\n",
                "import urllib.request\n",
                "import tarfile\n",
                "import time\n",
                "from tqdm import tqdm\n",
                "\n",
                "# Dataset URLs dari iFood 2019 Official\n",
                "DATASET_URLS = {\n",
                "    'annotations': {\n",
                "        'url': 'https://food-x.s3.amazonaws.com/annot.tar',\n",
                "        'filename': 'annot.tar',\n",
                "        'size': '3 MB',\n",
                "        'md5': '0c632c543ceed0e70f0eb2db58eda3ab'\n",
                "    },\n",
                "    'train': {\n",
                "        'url': 'https://food-x.s3.amazonaws.com/train.tar',\n",
                "        'filename': 'train.tar',\n",
                "        'size': '2.3 GB',\n",
                "        'md5': '8e56440e365ee852dcb0953a9307e27f'\n",
                "    },\n",
                "    'val': {\n",
                "        'url': 'https://food-x.s3.amazonaws.com/val.tar',\n",
                "        'filename': 'val.tar',\n",
                "        'size': '231 MB',\n",
                "        'md5': 'fa9a4c1eb929835a0fe68734f4868d3b'\n",
                "    },\n",
                "    'test': {\n",
                "        'url': 'https://food-x.s3.amazonaws.com/test.tar',\n",
                "        'filename': 'test.tar',\n",
                "        'size': '548 MB',\n",
                "        'md5': '32479146dd081d38895e46bb93fed58f'\n",
                "    }\n",
                "}\n",
                "\n",
                "class DownloadProgressBar(tqdm):\n",
                "    def update_to(self, b=1, bsize=1, tsize=None):\n",
                "        if tsize is not None:\n",
                "            self.total = tsize\n",
                "        self.update(b * bsize - self.n)\n",
                "\n",
                "def download_file(url, output_path, desc):\n",
                "    \"\"\"Download file dengan progress bar\"\"\"\n",
                "    with DownloadProgressBar(unit='B', unit_scale=True, miniters=1, desc=desc) as t:\n",
                "        urllib.request.urlretrieve(url, filename=output_path, reporthook=t.update_to)\n",
                "\n",
                "# Download semua file\n",
                "print(\"üì• Mulai download dataset iFood 2019...\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "download_dir = '/content/downloads'\n",
                "os.makedirs(download_dir, exist_ok=True)\n",
                "\n",
                "for name, info in DATASET_URLS.items():\n",
                "    output_path = os.path.join(download_dir, info['filename'])\n",
                "    \n",
                "    # Skip jika sudah ada\n",
                "    if os.path.exists(output_path):\n",
                "        print(f\"‚è≠Ô∏è  {name}: sudah ada, skip download\")\n",
                "        continue\n",
                "    \n",
                "    print(f\"\\nüì• Downloading {name} ({info['size']})...\")\n",
                "    start_time = time.time()\n",
                "    \n",
                "    try:\n",
                "        download_file(info['url'], output_path, info['filename'])\n",
                "        elapsed = time.time() - start_time\n",
                "        print(f\"‚úÖ {name} selesai dalam {elapsed:.1f} detik\")\n",
                "    except Exception as e:\n",
                "        print(f\"‚ùå Error downloading {name}: {e}\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"‚úÖ Semua file berhasil didownload!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# STEP 3: Extract ke Google Drive\n",
                "# ============================================================\n",
                "\n",
                "import tarfile\n",
                "import shutil\n",
                "\n",
                "download_dir = '/content/downloads'\n",
                "DATASET_PATH = '/content/drive/MyDrive/AlexNet_iFood2019/dataset'\n",
                "\n",
                "print(\"üì¶ Mulai ekstraksi ke Google Drive...\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# 1. Extract annotations\n",
                "print(\"\\nüì¶ Extracting annotations...\")\n",
                "annot_tar = os.path.join(download_dir, 'annot.tar')\n",
                "if os.path.exists(annot_tar):\n",
                "    with tarfile.open(annot_tar, 'r') as tar:\n",
                "        tar.extractall(DATASET_PATH)\n",
                "    print(\"‚úÖ Annotations extracted\")\n",
                "\n",
                "# 2. Extract train images\n",
                "print(\"\\nüì¶ Extracting train images (ini akan memakan waktu ~10-15 menit)...\")\n",
                "train_tar = os.path.join(download_dir, 'train.tar')\n",
                "train_dir = os.path.join(DATASET_PATH, 'train_images')\n",
                "if os.path.exists(train_tar):\n",
                "    os.makedirs(train_dir, exist_ok=True)\n",
                "    with tarfile.open(train_tar, 'r') as tar:\n",
                "        # Extract to temp then move\n",
                "        tar.extractall('/content/temp_train')\n",
                "    # Move images to correct folder\n",
                "    src_dir = '/content/temp_train/train_set'\n",
                "    if os.path.exists(src_dir):\n",
                "        for f in os.listdir(src_dir):\n",
                "            shutil.move(os.path.join(src_dir, f), train_dir)\n",
                "        shutil.rmtree('/content/temp_train')\n",
                "    print(f\"‚úÖ Train images extracted: {len(os.listdir(train_dir))} files\")\n",
                "\n",
                "# 3. Extract val images\n",
                "print(\"\\nüì¶ Extracting validation images...\")\n",
                "val_tar = os.path.join(download_dir, 'val.tar')\n",
                "val_dir = os.path.join(DATASET_PATH, 'val_images')\n",
                "if os.path.exists(val_tar):\n",
                "    os.makedirs(val_dir, exist_ok=True)\n",
                "    with tarfile.open(val_tar, 'r') as tar:\n",
                "        tar.extractall('/content/temp_val')\n",
                "    src_dir = '/content/temp_val/val_set'\n",
                "    if os.path.exists(src_dir):\n",
                "        for f in os.listdir(src_dir):\n",
                "            shutil.move(os.path.join(src_dir, f), val_dir)\n",
                "        shutil.rmtree('/content/temp_val')\n",
                "    print(f\"‚úÖ Val images extracted: {len(os.listdir(val_dir))} files\")\n",
                "\n",
                "# 4. Extract test images\n",
                "print(\"\\nüì¶ Extracting test images...\")\n",
                "test_tar = os.path.join(download_dir, 'test.tar')\n",
                "test_dir = os.path.join(DATASET_PATH, 'test_images')\n",
                "if os.path.exists(test_tar):\n",
                "    os.makedirs(test_dir, exist_ok=True)\n",
                "    with tarfile.open(test_tar, 'r') as tar:\n",
                "        tar.extractall('/content/temp_test')\n",
                "    src_dir = '/content/temp_test/test_set'\n",
                "    if os.path.exists(src_dir):\n",
                "        for f in os.listdir(src_dir):\n",
                "            shutil.move(os.path.join(src_dir, f), test_dir)\n",
                "        shutil.rmtree('/content/temp_test')\n",
                "    print(f\"‚úÖ Test images extracted: {len(os.listdir(test_dir))} files\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"‚úÖ Ekstraksi selesai!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# STEP 4: Verifikasi Dataset\n",
                "# ============================================================\n",
                "\n",
                "import os\n",
                "\n",
                "DATASET_PATH = '/content/drive/MyDrive/AlexNet_iFood2019/dataset'\n",
                "\n",
                "print(\"üîç Verifikasi dataset...\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# Check files\n",
                "required_items = {\n",
                "    'class_list.txt': 'file',\n",
                "    'train_info.csv': 'file',\n",
                "    'val_info.csv': 'file',\n",
                "    'test_info.csv': 'file',\n",
                "    'train_images': 'dir',\n",
                "    'val_images': 'dir',\n",
                "    'test_images': 'dir'\n",
                "}\n",
                "\n",
                "all_ok = True\n",
                "for item, item_type in required_items.items():\n",
                "    path = os.path.join(DATASET_PATH, item)\n",
                "    \n",
                "    if item_type == 'file':\n",
                "        exists = os.path.isfile(path)\n",
                "    else:\n",
                "        exists = os.path.isdir(path)\n",
                "    \n",
                "    if exists:\n",
                "        if item_type == 'dir':\n",
                "            count = len(os.listdir(path))\n",
                "            print(f\"‚úÖ {item}: {count:,} files\")\n",
                "        else:\n",
                "            print(f\"‚úÖ {item}\")\n",
                "    else:\n",
                "        print(f\"‚ùå {item}: TIDAK DITEMUKAN\")\n",
                "        all_ok = False\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "if all_ok:\n",
                "    print(\"üéâ DATASET SIAP DIGUNAKAN!\")\n",
                "    print(f\"\\nLokasi: {DATASET_PATH}\")\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è  Ada file yang hilang, coba jalankan ulang dari Step 2\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# STEP 5: Cleanup (Hapus file tar untuk hemat storage)\n",
                "# ============================================================\n",
                "\n",
                "import shutil\n",
                "\n",
                "download_dir = '/content/downloads'\n",
                "\n",
                "if os.path.exists(download_dir):\n",
                "    size_before = sum(os.path.getsize(os.path.join(download_dir, f)) \n",
                "                      for f in os.listdir(download_dir) \n",
                "                      if os.path.isfile(os.path.join(download_dir, f)))\n",
                "    \n",
                "    shutil.rmtree(download_dir)\n",
                "    print(f\"üóëÔ∏è  Deleted download cache: {size_before / 1e9:.2f} GB freed\")\n",
                "else:\n",
                "    print(\"‚úÖ No cache to clean\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"üéâ SELESAI!\")\n",
                "print(\"=\"*60)\n",
                "print(\"\\nDataset sudah tersimpan di Google Drive.\")\n",
                "print(\"Anda bisa menutup notebook ini dan lanjut ke training.\")\n",
                "print(\"\\nNotebook selanjutnya:\")\n",
                "print(\"  - train_member1_baseline.ipynb (Member 1)\")\n",
                "print(\"  - train_member2_mod1.ipynb (Member 2)\")\n",
                "print(\"  - train_member3_mod2.ipynb (Member 3)\")\n",
                "print(\"  - train_member4_combined.ipynb (Member 4)\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "accelerator": "GPU"
    },
    "nbformat": 4,
    "nbformat_minor": 4
}