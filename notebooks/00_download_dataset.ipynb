{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cb0f475-1692-4467-cfc1-13d63d2782ab",
        "id": "IL1fwaC2MxAe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Verifikasi dataset...\n",
            "============================================================\n",
            "‚úÖ class_list.txt\n",
            "‚úÖ train_info.csv\n",
            "‚úÖ val_info.csv\n",
            "‚úÖ test_info.csv\n",
            "‚úÖ train_images: 118,488 files\n",
            "‚úÖ val_images: 12,000 files\n",
            "‚úÖ test_images: 28,378 files\n",
            "\n",
            "============================================================\n",
            "üîç Verifikasi file gambar berdasarkan info CSV...\n",
            "  Mengecek 118474 gambar di train_images...\n",
            "  ‚ùå Error saat memverifikasi train_images: 'image_id'\n",
            "  Mengecek 11993 gambar di val_images...\n",
            "  ‚ùå Error saat memverifikasi val_images: 'image_id'\n",
            "  Mengecek 28376 gambar di test_images...\n",
            "  ‚ùå Error saat memverifikasi test_images: 'image_id'\n",
            "\n",
            "============================================================\n",
            "‚ö†Ô∏è  Ada file yang hilang atau bermasalah.\n",
            "Silakan coba langkah-langkah berikut:\n",
            "  * Jalankan ulang dari Step 2: Download Dataset.\n",
            "  * Jika masalah berlanjut, periksa koneksi internet Anda atau coba di waktu lain.\n",
            "  * Periksa log ekstraksi di Step 3 untuk memastikan tidak ada error.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "DATASET_PATH = '/content/drive/MyDrive/AlexNet_iFood2019/dataset'\n",
        "\n",
        "print(\"üîç Verifikasi dataset...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Check files\n",
        "required_items = {\n",
        "    'class_list.txt': 'file',\n",
        "    'train_info.csv': 'file',\n",
        "    'val_info.csv': 'file',\n",
        "    'test_info.csv': 'file',\n",
        "    'train_images': 'dir',\n",
        "    'val_images': 'dir',\n",
        "    'test_images': 'dir'\n",
        "}\n",
        "\n",
        "all_ok = True\n",
        "for item, item_type in required_items.items():\n",
        "    path = os.path.join(DATASET_PATH, item)\n",
        "\n",
        "    if item_type == 'file':\n",
        "        exists = os.path.isfile(path)\n",
        "    else:\n",
        "        exists = os.path.isdir(path)\n",
        "\n",
        "    if exists:\n",
        "        if item_type == 'dir':\n",
        "            count = len(os.listdir(path))\n",
        "            print(f\"‚úÖ {item}: {count:,} files\")\n",
        "        else:\n",
        "            print(f\"‚úÖ {item}\")\n",
        "    else:\n",
        "        print(f\"‚ùå {item}: TIDAK DITEMUKAN\")\n",
        "        all_ok = False\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "# --- New: Verify image files based on info CSVs ---\n",
        "print(\"üîç Verifikasi file gambar berdasarkan info CSV...\")\n",
        "\n",
        "def verify_image_paths(info_csv_path, images_dir_path, dataset_name):\n",
        "    global all_ok\n",
        "    try:\n",
        "        df = pd.read_csv(info_csv_path)\n",
        "        missing_count = 0\n",
        "        print(f\"  Mengecek {len(df)} gambar di {dataset_name}...\")\n",
        "        for index, row in df.iterrows():\n",
        "            image_filename = row['image_id'] + '.jpg'\n",
        "            image_path = os.path.join(images_dir_path, image_filename)\n",
        "            if not os.path.isfile(image_path):\n",
        "                print(f\"    ‚ùå File {image_filename} tidak ditemukan di {dataset_name}\")\n",
        "                missing_count += 1\n",
        "                all_ok = False\n",
        "        if missing_count == 0:\n",
        "            print(f\"  ‚úÖ Semua {len(df)} gambar di {dataset_name} ditemukan.\")\n",
        "        else:\n",
        "            print(f\"  ‚ö†Ô∏è  Total {missing_count} gambar hilang di {dataset_name}.\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"  ‚ùå Info CSV untuk {dataset_name} tidak ditemukan: {info_csv_path}\")\n",
        "        all_ok = False\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ùå Error saat memverifikasi {dataset_name}: {e}\")\n",
        "        all_ok = False\n",
        "\n",
        "if os.path.isfile(os.path.join(DATASET_PATH, 'train_info.csv')) and os.path.isdir(os.path.join(DATASET_PATH, 'train_images')):\n",
        "    verify_image_paths(os.path.join(DATASET_PATH, 'train_info.csv'), os.path.join(DATASET_PATH, 'train_images'), 'train_images')\n",
        "\n",
        "if os.path.isfile(os.path.join(DATASET_PATH, 'val_info.csv')) and os.path.isdir(os.path.join(DATASET_PATH, 'val_images')):\n",
        "    verify_image_paths(os.path.join(DATASET_PATH, 'val_info.csv'), os.path.join(DATASET_PATH, 'val_images'), 'val_images')\n",
        "\n",
        "if os.path.isfile(os.path.join(DATASET_PATH, 'test_info.csv')) and os.path.isdir(os.path.join(DATASET_PATH, 'test_images')):\n",
        "    verify_image_paths(os.path.join(DATASET_PATH, 'test_info.csv'), os.path.join(DATASET_PATH, 'test_images'), 'test_images')\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "if all_ok:\n",
        "    print(\"üéâ DATASET SIAP DIGUNAKAN!\")\n",
        "    print(f\"\\nLokasi: {DATASET_PATH}\")\n",
        "alternate_suggestion = \"\"\n",
        "if not os.path.exists(os.path.join(DATASET_PATH, 'train_images')) or len(os.listdir(os.path.join(DATASET_PATH, 'train_images'))) < 100000: # Heuristic for 'incomplete' train_images\n",
        "    alternate_suggestion = \"\\n  * Pastikan folder `train_images` terisi penuh. Terkadang proses ekstraksi dapat terganggu.\\n  * Periksa kembali ukuran `train.tar` di Google Drive Anda jika proses download tidak berhasil.\"\n",
        "\n",
        "if not all_ok:\n",
        "    print(\"‚ö†Ô∏è  Ada file yang hilang atau bermasalah.\\nSilakan coba langkah-langkah berikut:\")\n",
        "    print(\"  * Jalankan ulang dari Step 2: Download Dataset.\")\n",
        "    print(\"  * Jika masalah berlanjut, periksa koneksi internet Anda atau coba di waktu lain.\")\n",
        "    print(\"  * Periksa log ekstraksi di Step 3 untuk memastikan tidak ada error.\")\n",
        "    print(alternate_suggestion)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cb0f475-1692-4467-cfc1-13d63d2782ab",
        "id": "tEK00XX-MyVU"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Verifikasi dataset...\n",
            "============================================================\n",
            "‚úÖ class_list.txt\n",
            "‚úÖ train_info.csv\n",
            "‚úÖ val_info.csv\n",
            "‚úÖ test_info.csv\n",
            "‚úÖ train_images: 118,488 files\n",
            "‚úÖ val_images: 12,000 files\n",
            "‚úÖ test_images: 28,378 files\n",
            "\n",
            "============================================================\n",
            "üîç Verifikasi file gambar berdasarkan info CSV...\n",
            "  Mengecek 118474 gambar di train_images...\n",
            "  ‚ùå Error saat memverifikasi train_images: 'image_id'\n",
            "  Mengecek 11993 gambar di val_images...\n",
            "  ‚ùå Error saat memverifikasi val_images: 'image_id'\n",
            "  Mengecek 28376 gambar di test_images...\n",
            "  ‚ùå Error saat memverifikasi test_images: 'image_id'\n",
            "\n",
            "============================================================\n",
            "‚ö†Ô∏è  Ada file yang hilang atau bermasalah.\n",
            "Silakan coba langkah-langkah berikut:\n",
            "  * Jalankan ulang dari Step 2: Download Dataset.\n",
            "  * Jika masalah berlanjut, periksa koneksi internet Anda atau coba di waktu lain.\n",
            "  * Periksa log ekstraksi di Step 3 untuk memastikan tidak ada error.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "DATASET_PATH = '/content/drive/MyDrive/AlexNet_iFood2019/dataset'\n",
        "\n",
        "print(\"üîç Verifikasi dataset...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Check files\n",
        "required_items = {\n",
        "    'class_list.txt': 'file',\n",
        "    'train_info.csv': 'file',\n",
        "    'val_info.csv': 'file',\n",
        "    'test_info.csv': 'file',\n",
        "    'train_images': 'dir',\n",
        "    'val_images': 'dir',\n",
        "    'test_images': 'dir'\n",
        "}\n",
        "\n",
        "all_ok = True\n",
        "for item, item_type in required_items.items():\n",
        "    path = os.path.join(DATASET_PATH, item)\n",
        "\n",
        "    if item_type == 'file':\n",
        "        exists = os.path.isfile(path)\n",
        "    else:\n",
        "        exists = os.path.isdir(path)\n",
        "\n",
        "    if exists:\n",
        "        if item_type == 'dir':\n",
        "            count = len(os.listdir(path))\n",
        "            print(f\"‚úÖ {item}: {count:,} files\")\n",
        "        else:\n",
        "            print(f\"‚úÖ {item}\")\n",
        "    else:\n",
        "        print(f\"‚ùå {item}: TIDAK DITEMUKAN\")\n",
        "        all_ok = False\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "# --- New: Verify image files based on info CSVs ---\n",
        "print(\"üîç Verifikasi file gambar berdasarkan info CSV...\")\n",
        "\n",
        "def verify_image_paths(info_csv_path, images_dir_path, dataset_name):\n",
        "    global all_ok\n",
        "    try:\n",
        "        df = pd.read_csv(info_csv_path)\n",
        "        missing_count = 0\n",
        "        print(f\"  Mengecek {len(df)} gambar di {dataset_name}...\")\n",
        "        for index, row in df.iterrows():\n",
        "            image_filename = row['image_id'] + '.jpg'\n",
        "            image_path = os.path.join(images_dir_path, image_filename)\n",
        "            if not os.path.isfile(image_path):\n",
        "                print(f\"    ‚ùå File {image_filename} tidak ditemukan di {dataset_name}\")\n",
        "                missing_count += 1\n",
        "                all_ok = False\n",
        "        if missing_count == 0:\n",
        "            print(f\"  ‚úÖ Semua {len(df)} gambar di {dataset_name} ditemukan.\")\n",
        "        else:\n",
        "            print(f\"  ‚ö†Ô∏è  Total {missing_count} gambar hilang di {dataset_name}.\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"  ‚ùå Info CSV untuk {dataset_name} tidak ditemukan: {info_csv_path}\")\n",
        "        all_ok = False\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ùå Error saat memverifikasi {dataset_name}: {e}\")\n",
        "        all_ok = False\n",
        "\n",
        "if os.path.isfile(os.path.join(DATASET_PATH, 'train_info.csv')) and os.path.isdir(os.path.join(DATASET_PATH, 'train_images')):\n",
        "    verify_image_paths(os.path.join(DATASET_PATH, 'train_info.csv'), os.path.join(DATASET_PATH, 'train_images'), 'train_images')\n",
        "\n",
        "if os.path.isfile(os.path.join(DATASET_PATH, 'val_info.csv')) and os.path.isdir(os.path.join(DATASET_PATH, 'val_images')):\n",
        "    verify_image_paths(os.path.join(DATASET_PATH, 'val_info.csv'), os.path.join(DATASET_PATH, 'val_images'), 'val_images')\n",
        "\n",
        "if os.path.isfile(os.path.join(DATASET_PATH, 'test_info.csv')) and os.path.isdir(os.path.join(DATASET_PATH, 'test_images')):\n",
        "    verify_image_paths(os.path.join(DATASET_PATH, 'test_info.csv'), os.path.join(DATASET_PATH, 'test_images'), 'test_images')\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "if all_ok:\n",
        "    print(\"üéâ DATASET SIAP DIGUNAKAN!\")\n",
        "    print(f\"\\nLokasi: {DATASET_PATH}\")\n",
        "alternate_suggestion = \"\"\n",
        "if not os.path.exists(os.path.join(DATASET_PATH, 'train_images')) or len(os.listdir(os.path.join(DATASET_PATH, 'train_images'))) < 100000: # Heuristic for 'incomplete' train_images\n",
        "    alternate_suggestion = \"\\n  * Pastikan folder `train_images` terisi penuh. Terkadang proses ekstraksi dapat terganggu.\\n  * Periksa kembali ukuran `train.tar` di Google Drive Anda jika proses download tidak berhasil.\"\n",
        "\n",
        "if not all_ok:\n",
        "    print(\"‚ö†Ô∏è  Ada file yang hilang atau bermasalah.\\nSilakan coba langkah-langkah berikut:\")\n",
        "    print(\"  * Jalankan ulang dari Step 2: Download Dataset.\")\n",
        "    print(\"  * Jika masalah berlanjut, periksa koneksi internet Anda atau coba di waktu lain.\")\n",
        "    print(\"  * Periksa log ekstraksi di Step 3 untuk memastikan tidak ada error.\")\n",
        "    print(alternate_suggestion)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gcjCNTsgxrB"
      },
      "source": [
        "# üì• Download Dataset iFood 2019 - Otomatis\n",
        "\n",
        "Notebook ini akan **otomatis mendownload dan mengekstrak** dataset iFood 2019 ke Google Drive.\n",
        "\n",
        "### Dataset Info:\n",
        "| File | Size | Isi |\n",
        "|------|------|-----|\n",
        "| Annotations | 3 MB | Labels & class list |\n",
        "| Train Images | 2.3 GB | 120,216 gambar |\n",
        "| Val Images | 231 MB | 12,170 gambar |\n",
        "| Test Images | 548 MB | 28,399 gambar |\n",
        "\n",
        "**Total: ~3.1 GB**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUhfh_XqgxrE",
        "outputId": "27cd3c63-78f2-4543-f529-ee04461936b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "‚úÖ Google Drive mounted\n",
            "üìÅ Dataset akan disimpan di: /content/drive/MyDrive/AlexNet_iFood2019/dataset\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# STEP 1: Mount Google Drive\n",
        "# ============================================================\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "\n",
        "# Buat folder struktur\n",
        "PROJECT_PATH = '/content/drive/MyDrive/AlexNet_iFood2019'\n",
        "DATASET_PATH = os.path.join(PROJECT_PATH, 'dataset')\n",
        "\n",
        "os.makedirs(DATASET_PATH, exist_ok=True)\n",
        "os.makedirs(os.path.join(PROJECT_PATH, 'checkpoints'), exist_ok=True)\n",
        "os.makedirs(os.path.join(PROJECT_PATH, 'evaluation_results'), exist_ok=True)\n",
        "os.makedirs(os.path.join(PROJECT_PATH, 'analysis_results'), exist_ok=True)\n",
        "\n",
        "print(f\"‚úÖ Google Drive mounted\")\n",
        "print(f\"üìÅ Dataset akan disimpan di: {DATASET_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWc4hPdBgxrH",
        "outputId": "b53a498e-40ba-4775-cec2-41726db41d05"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì• Mulai download dataset iFood 2019...\n",
            "============================================================\n",
            "‚è≠Ô∏è  annotations: sudah ada. Memverifikasi MD5...\n",
            "‚úÖ MD5 cocok. annotations sudah siap.\n",
            "‚è≠Ô∏è  train: sudah ada. Memverifikasi MD5...\n",
            "‚úÖ MD5 cocok. train sudah siap.\n",
            "‚è≠Ô∏è  val: sudah ada. Memverifikasi MD5...\n",
            "‚úÖ MD5 cocok. val sudah siap.\n",
            "‚è≠Ô∏è  test: sudah ada. Memverifikasi MD5...\n",
            "‚úÖ MD5 cocok. test sudah siap.\n",
            "\n",
            "============================================================\n",
            "‚úÖ Semua file berhasil didownload dan diverifikasi!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import urllib.request\n",
        "import tarfile\n",
        "import time\n",
        "import hashlib\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Dataset URLs dari iFood 2019 Official\n",
        "DATASET_URLS = {\n",
        "    'annotations': {\n",
        "        'url': 'https://food-x.s3.amazonaws.com/annot.tar',\n",
        "        'filename': 'annot.tar',\n",
        "        'size': '3 MB',\n",
        "        'md5': '0c632c543ceed0e70f0eb2db58eda3ab'\n",
        "    },\n",
        "    'train': {\n",
        "        'url': 'https://food-x.s3.amazonaws.com/train.tar',\n",
        "        'filename': 'train.tar',\n",
        "        'size': '2.3 GB',\n",
        "        'md5': '8e56440e365ee852dcb0953a9307e27f'\n",
        "    },\n",
        "    'val': {\n",
        "        'url': 'https://food-x.s3.amazonaws.com/val.tar',\n",
        "        'filename': 'val.tar',\n",
        "        'size': '231 MB',\n",
        "        'md5': 'fa9a4c1eb929835a0fe68734f4868d3b'\n",
        "    },\n",
        "    'test': {\n",
        "        'url': 'https://food-x.s3.amazonaws.com/test.tar',\n",
        "        'filename': 'test.tar',\n",
        "        'size': '548 MB',\n",
        "        'md5': '32479146dd081d38895e46bb93fed58f'\n",
        "    }\n",
        "}\n",
        "\n",
        "class DownloadProgressBar(tqdm):\n",
        "    def update_to(self, b=1, bsize=1, tsize=None):\n",
        "        if tsize is not None:\n",
        "            self.total = tsize\n",
        "        self.update(b * bsize - self.n)\n",
        "\n",
        "def calculate_md5(filepath, chunk_size=8192):\n",
        "    \"\"\"Menghitung MD5 checksum dari file.\"\"\"\n",
        "    md5 = hashlib.md5()\n",
        "    with open(filepath, 'rb') as f:\n",
        "        for chunk in iter(lambda: f.read(chunk_size), b''):\n",
        "            md5.update(chunk)\n",
        "    return md5.hexdigest()\n",
        "\n",
        "def download_file_with_retries(url, output_path, desc, expected_md5, max_retries=5, initial_delay=5):\n",
        "    \"\"\"Download file dengan progress bar, retries, dan verifikasi MD5.\"\"\"\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            print(f\"  Attempt {attempt + 1}/{max_retries} for {desc}...\")\n",
        "            with DownloadProgressBar(unit='B', unit_scale=True, miniters=1, desc=desc) as t:\n",
        "                urllib.request.urlretrieve(url, filename=output_path, reporthook=t.update_to)\n",
        "\n",
        "            # Verifikasi MD5\n",
        "            actual_md5 = calculate_md5(output_path)\n",
        "            if actual_md5 == expected_md5:\n",
        "                return True # Sukses download dan verifikasi\n",
        "            else:\n",
        "                print(f\"  ‚ùå MD5 checksum mismatch for {desc}. Expected {expected_md5}, got {actual_md5}.\")\n",
        "                os.remove(output_path) # Hapus file yang rusak\n",
        "                raise Exception(\"MD5 mismatch\")\n",
        "\n",
        "        except (urllib.error.URLError, Exception) as e:\n",
        "            print(f\"  ‚ö†Ô∏è  Error downloading {desc} (attempt {attempt + 1}): {e}\")\n",
        "            if attempt < max_retries - 1:\n",
        "                delay = initial_delay * (2 ** attempt) # Exponential backoff\n",
        "                print(f\"  Retrying in {delay} seconds...\")\n",
        "                time.sleep(delay)\n",
        "            else:\n",
        "                print(f\"  ‚ùå Failed to download {desc} after {max_retries} attempts.\")\n",
        "                return False # Gagal setelah semua percobaan\n",
        "    return False\n",
        "\n",
        "# Download semua file\n",
        "print(\"üì• Mulai download dataset iFood 2019...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "download_dir = '/content/downloads'\n",
        "os.makedirs(download_dir, exist_ok=True)\n",
        "\n",
        "all_downloads_successful = True\n",
        "\n",
        "for name, info in DATASET_URLS.items():\n",
        "    output_path = os.path.join(download_dir, info['filename'])\n",
        "\n",
        "    # Skip jika sudah ada dan MD5 cocok\n",
        "    if os.path.exists(output_path):\n",
        "        print(f\"‚è≠Ô∏è  {name}: sudah ada. Memverifikasi MD5...\")\n",
        "        try:\n",
        "            actual_md5 = calculate_md5(output_path)\n",
        "            if actual_md5 == info['md5']:\n",
        "                print(f\"‚úÖ MD5 cocok. {name} sudah siap.\")\n",
        "                continue\n",
        "            else:\n",
        "                print(f\"‚ö†Ô∏è  MD5 mismatch for existing {name}. Redownloading... (Expected {info['md5']}, got {actual_md5})\")\n",
        "                os.remove(output_path) # Hapus file yang rusak\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è  Error checking existing file {name}: {e}. Redownloading...\")\n",
        "            if os.path.exists(output_path): os.remove(output_path)\n",
        "\n",
        "    print(f\"\\nüì• Downloading {name} ({info['size']})...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    success = download_file_with_retries(\n",
        "        info['url'], output_path, info['filename'], info['md5']\n",
        "    )\n",
        "\n",
        "    if success:\n",
        "        elapsed = time.time() - start_time\n",
        "        print(f\"‚úÖ {name} selesai dalam {elapsed:.1f} detik\")\n",
        "    else:\n",
        "        all_downloads_successful = False\n",
        "        print(f\"‚ùå Gagal mendownload {name}. Proses mungkin tidak lengkap.\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "if all_downloads_successful:\n",
        "    print(\"‚úÖ Semua file berhasil didownload dan diverifikasi!\")\n",
        "else:\n",
        "    print(\"‚ùå Ada beberapa file yang gagal didownload atau diverifikasi. Mohon periksa log di atas.\")\n",
        "    print(\"Disarankan untuk mengulang kembali proses download.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBfLPqcbgxrJ",
        "outputId": "897dfd32-fa4c-4dd8-c486-adab893824d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì¶ Mulai ekstraksi ke Google Drive...\n",
            "============================================================\n",
            "\n",
            "üì¶ Extracting annotations...\n",
            "‚úÖ Annotations extracted\n",
            "\n",
            "üì¶ Extracting train images (ini akan memakan waktu ~10-15 menit)...\n",
            "   Extracting to temp...\n",
            "   Moving to Drive...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Moving files to train_images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 118475/118475 [05:30<00:00, 358.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ‚ÑπÔ∏è  Skipped 102335 existing files.\n",
            "‚úÖ Train images: 16140 files moved, 102335 skipped, 0 failed. Total in target: 118488 files.\n",
            "\n",
            "üì¶ Extracting validation images...\n",
            "   Extracting to temp...\n",
            "   Moving to Drive...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Moving files to val_images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11994/11994 [04:49<00:00, 41.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ‚ÑπÔ∏è  Skipped 496 existing files.\n",
            "‚úÖ Val images: 11498 files moved, 496 skipped, 0 failed. Total in target: 12000 files.\n",
            "\n",
            "üì¶ Extracting test images...\n",
            "   Extracting to temp...\n",
            "   Moving to Drive...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Moving files to test_images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28377/28377 [12:18<00:00, 38.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ‚ÑπÔ∏è  Skipped 752 existing files.\n",
            "‚úÖ Test images: 27625 files moved, 752 skipped, 0 failed. Total in target: 28378 files.\n",
            "\n",
            "============================================================\n",
            "‚úÖ Ekstraksi selesai!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# STEP 3: Extract ke Google Drive (FIXED - handle existing files)\n",
        "# ============================================================\n",
        "\n",
        "import tarfile\n",
        "import shutil\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "download_dir = '/content/downloads'\n",
        "DATASET_PATH = '/content/drive/MyDrive/AlexNet_iFood2019/dataset'\n",
        "\n",
        "def safe_extract(tar_path, extract_to, filter_arg='data'):\n",
        "    \"\"\"Extract tar file dengan handling untuk Python 3.12+\"\"\"\n",
        "    with tarfile.open(tar_path, 'r') as tar:\n",
        "        # Gunakan filter untuk Python 3.12+ (menghilangkan warning)\n",
        "        try:\n",
        "            tar.extractall(extract_to, filter=filter_arg)\n",
        "        except TypeError:\n",
        "            # Fallback untuk Python versi lama\n",
        "            tar.extractall(extract_to)\n",
        "\n",
        "def move_files_safe(src_dir, dest_dir):\n",
        "    \"\"\"Move files dengan skip jika sudah ada dan laporkan status.\"\"\"\n",
        "    if not os.path.exists(src_dir):\n",
        "        return 0, 0, 0 # moved, skipped, failed\n",
        "\n",
        "    files = os.listdir(src_dir)\n",
        "    moved = 0\n",
        "    skipped = 0\n",
        "    failed_moves = 0\n",
        "\n",
        "    for f in tqdm(files, desc=f\"Moving files to {os.path.basename(dest_dir)}\"):\n",
        "        src_path = os.path.join(src_dir, f)\n",
        "        dest_path = os.path.join(dest_dir, f)\n",
        "\n",
        "        if os.path.exists(dest_path):\n",
        "            skipped += 1\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            shutil.move(src_path, dest_path)\n",
        "            moved += 1\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not move {f} to {dest_dir}: {e}\")\n",
        "            failed_moves += 1\n",
        "\n",
        "    if skipped > 0:\n",
        "        print(f\"   ‚ÑπÔ∏è  Skipped {skipped} existing files.\")\n",
        "    if failed_moves > 0:\n",
        "        print(f\"   ‚ùå Failed to move {failed_moves} files.\")\n",
        "\n",
        "    return moved, skipped, failed_moves\n",
        "\n",
        "print(\"üì¶ Mulai ekstraksi ke Google Drive...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# 1. Extract annotations\n",
        "print(\"\\nüì¶ Extracting annotations...\")\n",
        "annot_tar = os.path.join(download_dir, 'annot.tar')\n",
        "if os.path.exists(annot_tar):\n",
        "    safe_extract(annot_tar, DATASET_PATH)\n",
        "    print(\"‚úÖ Annotations extracted\")\n",
        "else:\n",
        "    print(\"‚è≠Ô∏è  Annotations tar not found, skipping\")\n",
        "\n",
        "# 2. Extract train images\n",
        "print(\"\\nüì¶ Extracting train images (ini akan memakan waktu ~10-15 menit)...\")\n",
        "train_tar = os.path.join(download_dir, 'train.tar')\n",
        "train_dir = os.path.join(DATASET_PATH, 'train_images')\n",
        "temp_train = '/content/temp_train'\n",
        "\n",
        "if os.path.exists(train_tar):\n",
        "    os.makedirs(train_dir, exist_ok=True)\n",
        "\n",
        "    # Clean temp folder if exists\n",
        "    if os.path.exists(temp_train):\n",
        "        shutil.rmtree(temp_train)\n",
        "\n",
        "    print(\"   Extracting to temp...\")\n",
        "    safe_extract(train_tar, temp_train)\n",
        "\n",
        "    # Find the extracted folder\n",
        "    src_dir = os.path.join(temp_train, 'train_set')\n",
        "    if not os.path.exists(src_dir):\n",
        "        # Try alternative path\n",
        "        subdirs = os.listdir(temp_train)\n",
        "        if subdirs:\n",
        "            src_dir = os.path.join(temp_train, subdirs[0])\n",
        "\n",
        "    if os.path.exists(src_dir):\n",
        "        print(\"   Moving to Drive...\")\n",
        "        moved, skipped, failed = move_files_safe(src_dir, train_dir)\n",
        "        shutil.rmtree(temp_train, ignore_errors=True)\n",
        "        current_file_count = len(os.listdir(train_dir)) if os.path.exists(train_dir) else 0\n",
        "        print(f\"‚úÖ Train images: {moved} files moved, {skipped} skipped, {failed} failed. Total in target: {current_file_count} files.\")\n",
        "    else:\n",
        "        print(\"‚ùå Could not find train_set folder in temp extraction.\")\n",
        "else:\n",
        "    current_file_count = len(os.listdir(train_dir)) if os.path.exists(train_dir) else 0\n",
        "    print(f\"‚è≠Ô∏è  Train tar not found. Checking existing: {current_file_count} files.\")\n",
        "\n",
        "# 3. Extract val images\n",
        "print(\"\\nüì¶ Extracting validation images...\")\n",
        "val_tar = os.path.join(download_dir, 'val.tar')\n",
        "val_dir = os.path.join(DATASET_PATH, 'val_images')\n",
        "temp_val = '/content/temp_val'\n",
        "\n",
        "if os.path.exists(val_tar):\n",
        "    os.makedirs(val_dir, exist_ok=True)\n",
        "\n",
        "    if os.path.exists(temp_val):\n",
        "        shutil.rmtree(temp_val)\n",
        "\n",
        "    print(\"   Extracting to temp...\")\n",
        "    safe_extract(val_tar, temp_val)\n",
        "\n",
        "    src_dir = os.path.join(temp_val, 'val_set')\n",
        "    if not os.path.exists(src_dir):\n",
        "        subdirs = os.listdir(temp_val)\n",
        "        if subdirs:\n",
        "            src_dir = os.path.join(temp_val, subdirs[0])\n",
        "\n",
        "    if os.path.exists(src_dir):\n",
        "        print(\"   Moving to Drive...\")\n",
        "        moved, skipped, failed = move_files_safe(src_dir, val_dir)\n",
        "        shutil.rmtree(temp_val, ignore_errors=True)\n",
        "        current_file_count = len(os.listdir(val_dir)) if os.path.exists(val_dir) else 0\n",
        "        print(f\"‚úÖ Val images: {moved} files moved, {skipped} skipped, {failed} failed. Total in target: {current_file_count} files.\")\n",
        "    else:\n",
        "        print(\"‚ùå Could not find val_set folder in temp extraction.\")\n",
        "else:\n",
        "    current_file_count = len(os.listdir(val_dir)) if os.path.exists(val_dir) else 0\n",
        "    print(f\"‚è≠Ô∏è  Val tar not found. Checking existing: {current_file_count} files.\")\n",
        "\n",
        "# 4. Extract test images\n",
        "print(\"\\nüì¶ Extracting test images...\")\n",
        "test_tar = os.path.join(download_dir, 'test.tar')\n",
        "test_dir = os.path.join(DATASET_PATH, 'test_images')\n",
        "temp_test = '/content/temp_test'\n",
        "\n",
        "if os.path.exists(test_tar):\n",
        "    os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "    if os.path.exists(temp_test):\n",
        "        shutil.rmtree(temp_test)\n",
        "\n",
        "    print(\"   Extracting to temp...\")\n",
        "    safe_extract(test_tar, temp_test)\n",
        "\n",
        "    src_dir = os.path.join(temp_test, 'test_set')\n",
        "    if not os.path.exists(src_dir):\n",
        "        subdirs = os.listdir(temp_test)\n",
        "        if subdirs:\n",
        "            src_dir = os.path.join(temp_test, subdirs[0])\n",
        "\n",
        "    if os.path.exists(src_dir):\n",
        "        print(\"   Moving to Drive...\")\n",
        "        moved, skipped, failed = move_files_safe(src_dir, test_dir)\n",
        "        shutil.rmtree(temp_test, ignore_errors=True)\n",
        "        current_file_count = len(os.listdir(test_dir)) if os.path.exists(test_dir) else 0\n",
        "        print(f\"‚úÖ Test images: {moved} files moved, {skipped} skipped, {failed} failed. Total in target: {current_file_count} files.\")\n",
        "    else:\n",
        "        print(\"‚ùå Could not find test_set folder in temp extraction.\")\n",
        "else:\n",
        "    current_file_count = len(os.listdir(test_dir)) if os.path.exists(test_dir) else 0\n",
        "    print(f\"‚è≠Ô∏è  Test tar not found. Checking existing: {current_file_count} files.\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"‚úÖ Ekstraksi selesai!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zMcKYGagxrK",
        "outputId": "65be90ca-64d6-47d5-ed8b-8e6cc677fe6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Verifikasi dataset...\n",
            "============================================================\n",
            "‚úÖ class_list.txt\n",
            "‚úÖ train_info.csv\n",
            "‚úÖ val_info.csv\n",
            "‚úÖ test_info.csv\n",
            "‚úÖ train_images: 118,488 files\n",
            "‚úÖ val_images: 12,000 files\n",
            "‚úÖ test_images: 28,378 files\n",
            "\n",
            "============================================================\n",
            "üîç Verifikasi file gambar berdasarkan info CSV...\n",
            "  Mengecek 118475 gambar di train_images...\n",
            "  ‚úÖ Semua 118475 gambar di train_images ditemukan.\n",
            "  Mengecek 11994 gambar di val_images...\n",
            "  ‚úÖ Semua 11994 gambar di val_images ditemukan.\n",
            "  Mengecek 28377 gambar di test_images...\n",
            "  ‚úÖ Semua 28377 gambar di test_images ditemukan.\n",
            "\n",
            "============================================================\n",
            "üéâ DATASET SIAP DIGUNAKAN!\n",
            "\n",
            "Lokasi: /content/drive/MyDrive/AlexNet_iFood2019/dataset\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "DATASET_PATH = '/content/drive/MyDrive/AlexNet_iFood2019/dataset'\n",
        "\n",
        "print(\"üîç Verifikasi dataset...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Check files\n",
        "required_items = {\n",
        "    'class_list.txt': 'file',\n",
        "    'train_info.csv': 'file',\n",
        "    'val_info.csv': 'file',\n",
        "    'test_info.csv': 'file',\n",
        "    'train_images': 'dir',\n",
        "    'val_images': 'dir',\n",
        "    'test_images': 'dir'\n",
        "}\n",
        "\n",
        "all_ok = True\n",
        "for item, item_type in required_items.items():\n",
        "    path = os.path.join(DATASET_PATH, item)\n",
        "\n",
        "    if item_type == 'file':\n",
        "        exists = os.path.isfile(path)\n",
        "    else:\n",
        "        exists = os.path.isdir(path)\n",
        "\n",
        "    if exists:\n",
        "        if item_type == 'dir':\n",
        "            count = len(os.listdir(path))\n",
        "            print(f\"‚úÖ {item}: {count:,} files\")\n",
        "        else:\n",
        "            print(f\"‚úÖ {item}\")\n",
        "    else:\n",
        "        print(f\"‚ùå {item}: TIDAK DITEMUKAN\")\n",
        "        all_ok = False\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "# --- New: Verify image files based on info CSVs ---\n",
        "print(\"üîç Verifikasi file gambar berdasarkan info CSV...\")\n",
        "\n",
        "def verify_image_paths(info_csv_path, images_dir_path, dataset_name):\n",
        "    global all_ok\n",
        "    try:\n",
        "        # Read CSV without header, and assume image filename is in the first column (index 0)\n",
        "        df = pd.read_csv(info_csv_path, header=None)\n",
        "        missing_count = 0\n",
        "        print(f\"  Mengecek {len(df)} gambar di {dataset_name}...\")\n",
        "        for index, row in df.iterrows():\n",
        "            # The first column contains the full image filename (e.g., 'train_101733.jpg')\n",
        "            image_filename = str(row[0])\n",
        "            image_path = os.path.join(images_dir_path, image_filename)\n",
        "            if not os.path.isfile(image_path):\n",
        "                print(f\"    ‚ùå File {image_filename} tidak ditemukan di {dataset_name}\")\n",
        "                missing_count += 1\n",
        "                all_ok = False\n",
        "        if missing_count == 0:\n",
        "            print(f\"  ‚úÖ Semua {len(df)} gambar di {dataset_name} ditemukan.\")\n",
        "        else:\n",
        "            print(f\"  ‚ö†Ô∏è  Total {missing_count} gambar hilang di {dataset_name}.\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"  ‚ùå Info CSV untuk {dataset_name} tidak ditemukan: {info_csv_path}\")\n",
        "        all_ok = False\n",
        "    except Exception as e:\n",
        "        # Catch any other exception during processing\n",
        "        print(f\"  ‚ùå Error saat memverifikasi {dataset_name}: {e}\")\n",
        "        if 'df' in locals(): # Only print if df was successfully created\n",
        "            print(f\"     Kolom yang tersedia: {df.columns.tolist()}\")\n",
        "        all_ok = False\n",
        "\n",
        "if os.path.isfile(os.path.join(DATASET_PATH, 'train_info.csv')) and os.path.isdir(os.path.join(DATASET_PATH, 'train_images')):\n",
        "    verify_image_paths(os.path.join(DATASET_PATH, 'train_info.csv'), os.path.join(DATASET_PATH, 'train_images'), 'train_images')\n",
        "\n",
        "if os.path.isfile(os.path.join(DATASET_PATH, 'val_info.csv')) and os.path.isdir(os.path.join(DATASET_PATH, 'val_images')):\n",
        "    verify_image_paths(os.path.join(DATASET_PATH, 'val_info.csv'), os.path.join(DATASET_PATH, 'val_images'), 'val_images')\n",
        "\n",
        "if os.path.isfile(os.path.join(DATASET_PATH, 'test_info.csv')) and os.path.isdir(os.path.join(DATASET_PATH, 'test_images')):\n",
        "    verify_image_paths(os.path.join(DATASET_PATH, 'test_info.csv'), os.path.join(DATASET_PATH, 'test_images'), 'test_images')\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "if all_ok:\n",
        "    print(\"üéâ DATASET SIAP DIGUNAKAN!\")\n",
        "    print(f\"\\nLokasi: {DATASET_PATH}\")\n",
        "alternate_suggestion = \"\"\n",
        "if not os.path.exists(os.path.join(DATASET_PATH, 'train_images')) or len(os.listdir(os.path.join(DATASET_PATH, 'train_images'))) < 100000: # Heuristic for 'incomplete' train_images\n",
        "    alternate_suggestion = \"\\n  * Pastikan folder `train_images` terisi penuh. Terkadang proses ekstraksi dapat terganggu.\\n  * Periksa kembali ukuran `train.tar` di Google Drive Anda jika proses download tidak berhasil.\"\n",
        "\n",
        "if not all_ok:\n",
        "    print(\"‚ö†Ô∏è  Ada file yang hilang atau bermasalah.\\nSilakan coba langkah-langkah berikut:\")\n",
        "    print(\"  * Jalankan ulang dari Step 2: Download Dataset.\")\n",
        "    print(\"  * Jika masalah berlanjut, periksa koneksi internet Anda atau coba di waktu lain.\")\n",
        "    print(\"  * Periksa log ekstraksi di Step 3 untuk memastikan tidak ada error.\")\n",
        "    print(alternate_suggestion)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xgf_d3BHgxrK"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# STEP 5: Cleanup (Hapus file tar untuk hemat storage)\n",
        "# ============================================================\n",
        "\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "download_dir = '/content/downloads'\n",
        "\n",
        "if os.path.exists(download_dir):\n",
        "    size_before = sum(os.path.getsize(os.path.join(download_dir, f))\n",
        "                      for f in os.listdir(download_dir)\n",
        "                      if os.path.isfile(os.path.join(download_dir, f)))\n",
        "\n",
        "    shutil.rmtree(download_dir)\n",
        "    print(f\"üóëÔ∏è  Deleted download cache: {size_before / 1e9:.2f} GB freed\")\n",
        "else:\n",
        "    print(\"‚úÖ No cache to clean\")\n",
        "\n",
        "# Cleanup temp folders juga\n",
        "for temp in ['/content/temp_train', '/content/temp_val', '/content/temp_test']:\n",
        "    if os.path.exists(temp):\n",
        "        shutil.rmtree(temp, ignore_errors=True)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üéâ SELESAI!\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nDataset sudah tersimpan di Google Drive.\")\n",
        "print(\"Anda bisa menutup notebook ini dan lanjut ke training.\")\n",
        "print(\"\\nNotebook selanjutnya:\")\n",
        "print(\"  - train_member1_baseline.ipynb (Member 1)\")\n",
        "print(\"  - train_member2_mod1.ipynb (Member 2)\")\n",
        "print(\"  - train_member3_mod2.ipynb (Member 3)\")\n",
        "print(\"  - train_member4_combined.ipynb (Member 4)\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}