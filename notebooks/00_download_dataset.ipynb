{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üì• Download Dataset iFood 2019 - Otomatis\n",
                "\n",
                "Notebook ini akan **otomatis mendownload dan mengekstrak** dataset iFood 2019 ke Google Drive.\n",
                "\n",
                "### Dataset Info:\n",
                "| File | Size | Isi |\n",
                "|------|------|-----|\n",
                "| Annotations | 3 MB | Labels & class list |\n",
                "| Train Images | 2.3 GB | 120,216 gambar |\n",
                "| Val Images | 231 MB | 12,170 gambar |\n",
                "| Test Images | 548 MB | 28,399 gambar |\n",
                "\n",
                "**Total: ~3.1 GB**\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# STEP 1: Mount Google Drive\n",
                "# ============================================================\n",
                "\n",
                "from google.colab import drive\n",
                "drive.mount('/content/drive')\n",
                "\n",
                "import os\n",
                "\n",
                "# Buat folder struktur\n",
                "PROJECT_PATH = '/content/drive/MyDrive/AlexNet_iFood2019'\n",
                "DATASET_PATH = os.path.join(PROJECT_PATH, 'dataset')\n",
                "\n",
                "os.makedirs(DATASET_PATH, exist_ok=True)\n",
                "os.makedirs(os.path.join(PROJECT_PATH, 'checkpoints'), exist_ok=True)\n",
                "os.makedirs(os.path.join(PROJECT_PATH, 'evaluation_results'), exist_ok=True)\n",
                "os.makedirs(os.path.join(PROJECT_PATH, 'analysis_results'), exist_ok=True)\n",
                "\n",
                "print(f\"‚úÖ Google Drive mounted\")\n",
                "print(f\"üìÅ Dataset akan disimpan di: {DATASET_PATH}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# STEP 2: Download Dataset\n",
                "# ============================================================\n",
                "\n",
                "import os\n",
                "import urllib.request\n",
                "import tarfile\n",
                "import time\n",
                "from tqdm import tqdm\n",
                "\n",
                "# Dataset URLs dari iFood 2019 Official\n",
                "DATASET_URLS = {\n",
                "    'annotations': {\n",
                "        'url': 'https://food-x.s3.amazonaws.com/annot.tar',\n",
                "        'filename': 'annot.tar',\n",
                "        'size': '3 MB',\n",
                "        'md5': '0c632c543ceed0e70f0eb2db58eda3ab'\n",
                "    },\n",
                "    'train': {\n",
                "        'url': 'https://food-x.s3.amazonaws.com/train.tar',\n",
                "        'filename': 'train.tar',\n",
                "        'size': '2.3 GB',\n",
                "        'md5': '8e56440e365ee852dcb0953a9307e27f'\n",
                "    },\n",
                "    'val': {\n",
                "        'url': 'https://food-x.s3.amazonaws.com/val.tar',\n",
                "        'filename': 'val.tar',\n",
                "        'size': '231 MB',\n",
                "        'md5': 'fa9a4c1eb929835a0fe68734f4868d3b'\n",
                "    },\n",
                "    'test': {\n",
                "        'url': 'https://food-x.s3.amazonaws.com/test.tar',\n",
                "        'filename': 'test.tar',\n",
                "        'size': '548 MB',\n",
                "        'md5': '32479146dd081d38895e46bb93fed58f'\n",
                "    }\n",
                "}\n",
                "\n",
                "class DownloadProgressBar(tqdm):\n",
                "    def update_to(self, b=1, bsize=1, tsize=None):\n",
                "        if tsize is not None:\n",
                "            self.total = tsize\n",
                "        self.update(b * bsize - self.n)\n",
                "\n",
                "def download_file(url, output_path, desc):\n",
                "    \"\"\"Download file dengan progress bar\"\"\"\n",
                "    with DownloadProgressBar(unit='B', unit_scale=True, miniters=1, desc=desc) as t:\n",
                "        urllib.request.urlretrieve(url, filename=output_path, reporthook=t.update_to)\n",
                "\n",
                "# Download semua file\n",
                "print(\"üì• Mulai download dataset iFood 2019...\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "download_dir = '/content/downloads'\n",
                "os.makedirs(download_dir, exist_ok=True)\n",
                "\n",
                "for name, info in DATASET_URLS.items():\n",
                "    output_path = os.path.join(download_dir, info['filename'])\n",
                "    \n",
                "    # Skip jika sudah ada\n",
                "    if os.path.exists(output_path):\n",
                "        print(f\"‚è≠Ô∏è  {name}: sudah ada, skip download\")\n",
                "        continue\n",
                "    \n",
                "    print(f\"\\nüì• Downloading {name} ({info['size']})...\")\n",
                "    start_time = time.time()\n",
                "    \n",
                "    try:\n",
                "        download_file(info['url'], output_path, info['filename'])\n",
                "        elapsed = time.time() - start_time\n",
                "        print(f\"‚úÖ {name} selesai dalam {elapsed:.1f} detik\")\n",
                "    except Exception as e:\n",
                "        print(f\"‚ùå Error downloading {name}: {e}\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"‚úÖ Semua file berhasil didownload!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# STEP 3: Extract ke Google Drive\n",
                "# ============================================================\n",
                "\n",
                "import tarfile\n",
                "import shutil\n",
                "\n",
                "download_dir = '/content/downloads'\n",
                "DATASET_PATH = '/content/drive/MyDrive/AlexNet_iFood2019/dataset'\n",
                "\n",
                "print(\"üì¶ Mulai ekstraksi ke Google Drive...\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# 1. Extract annotations\n",
                "print(\"\\nüì¶ Extracting annotations...\")\n",
                "annot_tar = os.path.join(download_dir, 'annot.tar')\n",
                "if os.path.exists(annot_tar):\n",
                "    with tarfile.open(annot_tar, 'r') as tar:\n",
                "        tar.extractall(DATASET_PATH)\n",
                "    print(\"‚úÖ Annotations extracted\")\n",
                "\n",
                "# 2. Extract train images\n",
                "print(\"\\nüì¶ Extracting train images (ini akan memakan waktu ~10-15 menit)...\")\n",
                "train_tar = os.path.join(download_dir, 'train.tar')\n",
                "train_dir = os.path.join(DATASET_PATH, 'train_images')\n",
                "if os.path.exists(train_tar):\n",
                "    os.makedirs(train_dir, exist_ok=True)\n",
                "    with tarfile.open(train_tar, 'r') as tar:\n",
                "        # Extract to temp then move\n",
                "        tar.extractall('/content/temp_train')\n",
                "    # Move images to correct folder\n",
                "    src_dir = '/content/temp_train/train_set'\n",
                "    if os.path.exists(src_dir):\n",
                "        for f in os.listdir(src_dir):\n",
                "            shutil.move(os.path.join(src_dir, f), train_dir)\n",
                "        shutil.rmtree('/content/temp_train')\n",
                "    print(f\"‚úÖ Train images extracted: {len(os.listdir(train_dir))} files\")\n",
                "\n",
                "# 3. Extract val images\n",
                "print(\"\\nüì¶ Extracting validation images...\")\n",
                "val_tar = os.path.join(download_dir, 'val.tar')\n",
                "val_dir = os.path.join(DATASET_PATH, 'val_images')\n",
                "if os.path.exists(val_tar):\n",
                "    os.makedirs(val_dir, exist_ok=True)\n",
                "    with tarfile.open(val_tar, 'r') as tar:\n",
                "        tar.extractall('/content/temp_val')\n",
                "    src_dir = '/content/temp_val/val_set'\n",
                "    if os.path.exists(src_dir):\n",
                "        for f in os.listdir(src_dir):\n",
                "            shutil.move(os.path.join(src_dir, f), val_dir)\n",
                "        shutil.rmtree('/content/temp_val')\n",
                "    print(f\"‚úÖ Val images extracted: {len(os.listdir(val_dir))} files\")\n",
                "\n",
                "# 4. Extract test images\n",
                "print(\"\\nüì¶ Extracting test images...\")\n",
                "test_tar = os.path.join(download_dir, 'test.tar')\n",
                "test_dir = os.path.join(DATASET_PATH, 'test_images')\n",
                "if os.path.exists(test_tar):\n",
                "    os.makedirs(test_dir, exist_ok=True)\n",
                "    with tarfile.open(test_tar, 'r') as tar:\n",
                "        tar.extractall('/content/temp_test')\n",
                "    src_dir = '/content/temp_test/test_set'\n",
                "    if os.path.exists(src_dir):\n",
                "        for f in os.listdir(src_dir):\n",
                "            shutil.move(os.path.join(src_dir, f), test_dir)\n",
                "        shutil.rmtree('/content/temp_test')\n",
                "    print(f\"‚úÖ Test images extracted: {len(os.listdir(test_dir))} files\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"‚úÖ Ekstraksi selesai!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# STEP 4: Verifikasi Dataset\n",
                "# ============================================================\n",
                "\n",
                "import os\n",
                "\n",
                "DATASET_PATH = '/content/drive/MyDrive/AlexNet_iFood2019/dataset'\n",
                "\n",
                "print(\"üîç Verifikasi dataset...\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# Check files\n",
                "required_items = {\n",
                "    'class_list.txt': 'file',\n",
                "    'train_info.csv': 'file',\n",
                "    'val_info.csv': 'file',\n",
                "    'test_info.csv': 'file',\n",
                "    'train_images': 'dir',\n",
                "    'val_images': 'dir',\n",
                "    'test_images': 'dir'\n",
                "}\n",
                "\n",
                "all_ok = True\n",
                "for item, item_type in required_items.items():\n",
                "    path = os.path.join(DATASET_PATH, item)\n",
                "    \n",
                "    if item_type == 'file':\n",
                "        exists = os.path.isfile(path)\n",
                "    else:\n",
                "        exists = os.path.isdir(path)\n",
                "    \n",
                "    if exists:\n",
                "        if item_type == 'dir':\n",
                "            count = len(os.listdir(path))\n",
                "            print(f\"‚úÖ {item}: {count:,} files\")\n",
                "        else:\n",
                "            print(f\"‚úÖ {item}\")\n",
                "    else:\n",
                "        print(f\"‚ùå {item}: TIDAK DITEMUKAN\")\n",
                "        all_ok = False\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "if all_ok:\n",
                "    print(\"üéâ DATASET SIAP DIGUNAKAN!\")\n",
                "    print(f\"\\nLokasi: {DATASET_PATH}\")\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è  Ada file yang hilang, coba jalankan ulang dari Step 2\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "{\n",
                "  \"cells\": [\n",
                "    {\n",
                "      \"cell_type\": \"markdown\",\n",
                "      \"metadata\": {},\n",
                "      \"source\": [\n",
                "        \"# üì• Download Dataset iFood 2019 - Otomatis\\n\",\n",
                "        \"\\n\",\n",
                "        \"Notebook ini akan **otomatis mendownload dan mengekstrak** dataset iFood 2019 ke Google Drive.\\n\",\n",
                "        \"\\n\",\n",
                "        \"### Dataset Info:\\n\",\n",
                "        \"| File | Size | Isi |\\n\",\n",
                "        \"|------|------|-----|\\n\",\n",
                "        \"| Annotations | 3 MB | Labels & class list |\\n\",\n",
                "        \"| Train Images | 2.3 GB | 120,216 gambar |\\n\",\n",
                "        \"| Val Images | 231 MB | 12,170 gambar |\\n\",\n",
                "        \"| Test Images | 548 MB | 28,399 gambar |\\n\",\n",
                "        \"\\n\",\n",
                "        \"**Total: ~3.1 GB**\\n\",\n",
                "        \"\\n\",\n",
                "        \"---\"\n",
                "      ]\n",
                "    },\n",
                "    {\n",
                "      \"cell_type\": \"code\",\n",
                "      \"execution_count\": null,\n",
                "      \"metadata\": {},\n",
                "      \"outputs\": [],\n",
                "      \"source\": [\n",
                "        \"# ============================================================\\n\",\n",
                "        \"# STEP 1: Mount Google Drive\\n\",\n",
                "        \"# ============================================================\\n\",\n",
                "        \"\\n\",\n",
                "        \"from google.colab import drive\\n\",\n",
                "        \"drive.mount('/content/drive')\\n\",\n",
                "        \"\\n\",\n",
                "        \"import os\\n\",\n",
                "        \"\\n\",\n",
                "        \"# Buat folder struktur\\n\",\n",
                "        \"PROJECT_PATH = '/content/drive/MyDrive/AlexNet_iFood2019'\\n\",\n",
                "        \"DATASET_PATH = os.path.join(PROJECT_PATH, 'dataset')\\n\",\n",
                "        \"\\n\",\n",
                "        \"os.makedirs(DATASET_PATH, exist_ok=True)\\n\",\n",
                "        \"os.makedirs(os.path.join(PROJECT_PATH, 'checkpoints'), exist_ok=True)\\n\",\n",
                "        \"os.makedirs(os.path.join(PROJECT_PATH, 'evaluation_results'), exist_ok=True)\\n\",\n",
                "        \"os.makedirs(os.path.join(PROJECT_PATH, 'analysis_results'), exist_ok=True)\\n\",\n",
                "        \"\\n\",\n",
                "        \"print(f\\\"‚úÖ Google Drive mounted\\\")\\n\",\n",
                "        \"print(f\\\"üìÅ Dataset akan disimpan di: {DATASET_PATH}\\\")\"\n",
                "      ]\n",
                "    },\n",
                "    {\n",
                "      \"cell_type\": \"code\",\n",
                "      \"execution_count\": null,\n",
                "      \"metadata\": {},\n",
                "      \"outputs\": [],\n",
                "      \"source\": [\n",
                "        \"# ============================================================\\n\",\n",
                "        \"# STEP 2: Download Dataset\\n\",\n",
                "        \"# ============================================================\\n\",\n",
                "        \"\\n\",\n",
                "        \"import os\\n\",\n",
                "        \"import urllib.request\\n\",\n",
                "        \"import tarfile\\n\",\n",
                "        \"import time\\n\",\n",
                "        \"from tqdm import tqdm\\n\",\n",
                "        \"\\n\",\n",
                "        \"# Dataset URLs dari iFood 2019 Official\\n\",\n",
                "        \"DATASET_URLS = {\\n\",\n",
                "        \"    'annotations': {\\n\",\n",
                "        \"        'url': 'https://food-x.s3.amazonaws.com/annot.tar',\\n\",\n",
                "        \"        'filename': 'annot.tar',\\n\",\n",
                "        \"        'size': '3 MB',\\n\",\n",
                "        \"        'md5': '0c632c543ceed0e70f0eb2db58eda3ab'\\n\",\n",
                "        \"    },\\n\",\n",
                "        \"    'train': {\\n\",\n",
                "        \"        'url': 'https://food-x.s3.amazonaws.com/train.tar',\\n\",\n",
                "        \"        'filename': 'train.tar',\\n\",\n",
                "        \"        'size': '2.3 GB',\\n\",\n",
                "        \"        'md5': '8e56440e365ee852dcb0953a9307e27f'\\n\",\n",
                "        \"    },\\n\",\n",
                "        \"    'val': {\\n\",\n",
                "        \"        'url': 'https://food-x.s3.amazonaws.com/val.tar',\\n\",\n",
                "        \"        'filename': 'val.tar',\\n\",\n",
                "        \"        'size': '231 MB',\\n\",\n",
                "        \"        'md5': 'fa9a4c1eb929835a0fe68734f4868d3b'\\n\",\n",
                "        \"    },\\n\",\n",
                "        \"    'test': {\\n\",\n",
                "        \"        'url': 'https://food-x.s3.amazonaws.com/test.tar',\\n\",\n",
                "        \"        'filename': 'test.tar',\\n\",\n",
                "        \"        'size': '548 MB',\\n\",\n",
                "        \"        'md5': '32479146dd081d38895e46bb93fed58f'\\n\",\n",
                "        \"    }\\n\",\n",
                "        \"}\\n\",\n",
                "        \"\\n\",\n",
                "        \"class DownloadProgressBar(tqdm):\\n\",\n",
                "        \"    def update_to(self, b=1, bsize=1, tsize=None):\\n\",\n",
                "        \"        if tsize is not None:\\n\",\n",
                "        \"            self.total = tsize\\n\",\n",
                "        \"        self.update(b * bsize - self.n)\\n\",\n",
                "        \"\\n\",\n",
                "        \"def download_file(url, output_path, desc):\\n\",\n",
                "        \"    \\\"\\\"\\\"Download file dengan progress bar\\\"\\\"\\\"\\n\",\n",
                "        \"    with DownloadProgressBar(unit='B', unit_scale=True, miniters=1, desc=desc) as t:\\n\",\n",
                "        \"        urllib.request.urlretrieve(url, filename=output_path, reporthook=t.update_to)\\n\",\n",
                "        \"\\n\",\n",
                "        \"# Download semua file\\n\",\n",
                "        \"print(\\\"üì• Mulai download dataset iFood 2019...\\\")\\n\",\n",
                "        \"print(\\\"=\\\"*60)\\n\",\n",
                "        \"\\n\",\n",
                "        \"download_dir = '/content/downloads'\\n\",\n",
                "        \"os.makedirs(download_dir, exist_ok=True)\\n\",\n",
                "        \"\\n\",\n",
                "        \"for name, info in DATASET_URLS.items():\\n\",\n",
                "        \"    output_path = os.path.join(download_dir, info['filename'])\\n\",\n",
                "        \"    \\n\",\n",
                "        \"    # Skip jika sudah ada\\n\",\n",
                "        \"    if os.path.exists(output_path):\\n\",\n",
                "        \"        print(f\\\"‚è≠Ô∏è  {name}: sudah ada, skip download\\\")\\n\",\n",
                "        \"        continue\\n\",\n",
                "        \"    \\n\",\n",
                "        \"    print(f\\\"\\\\nüì• Downloading {name} ({info['size']})...\\\")\\n\",\n",
                "        \"    start_time = time.time()\\n\",\n",
                "        \"    \\n\",\n",
                "        \"    try:\\n\",\n",
                "        \"        download_file(info['url'], output_path, info['filename'])\\n\",\n",
                "        \"        elapsed = time.time() - start_time\\n\",\n",
                "        \"        print(f\\\"‚úÖ {name} selesai dalam {elapsed:.1f} detik\\\")\\n\",\n",
                "        \"    except Exception as e:\\n\",\n",
                "        \"        print(f\\\"‚ùå Error downloading {name}: {e}\\\")\\n\",\n",
                "        \"\\n\",\n",
                "        \"print(\\\"\\\\n\\\" + \\\"=\\\"*60)\\n\",\n",
                "        \"print(\\\"‚úÖ Semua file berhasil didownload!\\\")\"\n",
                "      ]\n",
                "    },\n",
                "    {\n",
                "      \"cell_type\": \"code\",\n",
                "      \"execution_count\": null,\n",
                "      \"metadata\": {},\n",
                "      \"outputs\": [],\n",
                "      \"source\": [\n",
                "        \"# ============================================================\\n\",\n",
                "        \"# STEP 3: Extract ke Google Drive (FIXED - handle existing files)\\n\",\n",
                "        \"# ============================================================\\n\",\n",
                "        \"\\n\",\n",
                "        \"import tarfile\\n\",\n",
                "        \"import shutil\\n\",\n",
                "        \"import os\\n\",\n",
                "        \"from tqdm import tqdm\\n\",\n",
                "        \"\\n\",\n",
                "        \"download_dir = '/content/downloads'\\n\",\n",
                "        \"DATASET_PATH = '/content/drive/MyDrive/AlexNet_iFood2019/dataset'\\n\",\n",
                "        \"\\n\",\n",
                "        \"def safe_extract(tar_path, extract_to, filter_arg='data'):\\n\",\n",
                "        \"    \\\"\\\"\\\"Extract tar file dengan handling untuk Python 3.12+\\\"\\\"\\\"\\n\",\n",
                "        \"    with tarfile.open(tar_path, 'r') as tar:\\n\",\n",
                "        \"        # Gunakan filter untuk Python 3.12+ (menghilangkan warning)\\n\",\n",
                "        \"        try:\\n\",\n",
                "        \"            tar.extractall(extract_to, filter=filter_arg)\\n\",\n",
                "        \"        except TypeError:\\n\",\n",
                "        \"            # Fallback untuk Python versi lama\\n\",\n",
                "        \"            tar.extractall(extract_to)\\n\",\n",
                "        \"\\n\",\n",
                "        \"def move_files_safe(src_dir, dest_dir):\\n\",\n",
                "        \"    \\\"\\\"\\\"Move files dengan skip jika sudah ada\\\"\\\"\\\"\\n\",\n",
                "        \"    if not os.path.exists(src_dir):\\n\",\n",
                "        \"        return 0\\n\",\n",
                "        \"    \\n\",\n",
                "        \"    files = os.listdir(src_dir)\\n\",\n",
                "        \"    moved = 0\\n\",\n",
                "        \"    skipped = 0\\n\",\n",
                "        \"    \\n\",\n",
                "        \"    for f in tqdm(files, desc=\\\"Moving files\\\"):\\n\",\n",
                "        \"        src_path = os.path.join(src_dir, f)\\n\",\n",
                "        \"        dest_path = os.path.join(dest_dir, f)\\n\",\n",
                "        \"        \\n\",\n",
                "        \"        if os.path.exists(dest_path):\\n\",\n",
                "        \"            # File sudah ada, skip\\n\",\n",
                "        \"            skipped += 1\\n\",\n",
                "        \"            continue\\n\",\n",
                "        \"        \\n\",\n",
                "        \"        try:\\n\",\n",
                "        \"            shutil.move(src_path, dest_path)\\n\",\n",
                "        \"            moved += 1\\n\",\n",
                "        \"        except Exception as e:\\n\",\n",
                "        \"            print(f\\\"Warning: Could not move {f}: {e}\\\")\\n\",\n",
                "        \"    \\n\",\n",
                "        \"    if skipped > 0:\\n\",\n",
                "        \"        print(f\\\"   ‚ÑπÔ∏è  Skipped {skipped} existing files\\\")\\n\",\n",
                "        \"    \\n\",\n",
                "        \"    return moved + skipped\\n\",\n",
                "        \"\\n\",\n",
                "        \"print(\\\"üì¶ Mulai ekstraksi ke Google Drive...\\\")\\n\",\n",
                "        \"print(\\\"=\\\"*60)\\n\",\n",
                "        \"\\n\",\n",
                "        \"# 1. Extract annotations\\n\",\n",
                "        \"print(\\\"\\\\nüì¶ Extracting annotations...\\\")\\n\",\n",
                "        \"annot_tar = os.path.join(download_dir, 'annot.tar')\\n\",\n",
                "        \"if os.path.exists(annot_tar):\\n\",\n",
                "        \"    safe_extract(annot_tar, DATASET_PATH)\\n\",\n",
                "        \"    print(\\\"‚úÖ Annotations extracted\\\")\\n\",\n",
                "        \"else:\\n\",\n",
                "        \"    print(\\\"‚è≠Ô∏è  Annotations tar not found, skipping\\\")\\n\",\n",
                "        \"\\n\",\n",
                "        \"# 2. Extract train images\\n\",\n",
                "        \"print(\\\"\\\\nüì¶ Extracting train images (ini akan memakan waktu ~10-15 menit)...\\\")\\n\",\n",
                "        \"train_tar = os.path.join(download_dir, 'train.tar')\\n\",\n",
                "        \"train_dir = os.path.join(DATASET_PATH, 'train_images')\\n\",\n",
                "        \"temp_train = '/content/temp_train'\\n\",\n",
                "        \"\\n\",\n",
                "        \"if os.path.exists(train_tar):\\n\",\n",
                "        \"    os.makedirs(train_dir, exist_ok=True)\\n\",\n",
                "        \"    \\n\",\n",
                "        \"    # Clean temp folder if exists\\n\",\n",
                "        \"    if os.path.exists(temp_train):\\n\",\n",
                "        \"        shutil.rmtree(temp_train)\\n\",\n",
                "        \"    \\n\",\n",
                "        \"    print(\\\"   Extracting to temp...\\\")\\n\",\n",
                "        \"    safe_extract(train_tar, temp_train)\\n\",\n",
                "        \"    \\n\",\n",
                "        \"    # Find the extracted folder\\n\",\n",
                "        \"    src_dir = os.path.join(temp_train, 'train_set')\\n\",\n",
                "        \"    if not os.path.exists(src_dir):\\n\",\n",
                "        \"        # Try alternative path\\n\",\n",
                "        \"        subdirs = os.listdir(temp_train)\\n\",\n",
                "        \"        if subdirs:\\n\",\n",
                "        \"            src_dir = os.path.join(temp_train, subdirs[0])\\n\",\n",
                "        \"    \\n\",\n",
                "        \"    if os.path.exists(src_dir):\\n\",\n",
                "        \"        print(\\\"   Moving to Drive...\\\")\\n\",\n",
                "        \"        count = move_files_safe(src_dir, train_dir)\\n\",\n",
                "        \"        shutil.rmtree(temp_train, ignore_errors=True)\\n\",\n",
                "        \"        print(f\\\"‚úÖ Train images: {len(os.listdir(train_dir))} files\\\")\\n\",\n",
                "        \"    else:\\n\",\n",
                "        \"        print(\\\"‚ùå Could not find train_set folder\\\")\\n\",\n",
                "        \"else:\\n\",\n",
                "        \"    print(f\\\"‚è≠Ô∏è  Train tar not found, checking existing: {len(os.listdir(train_dir)) if os.path.exists(train_dir) else 0} files\\\")\\n\",\n",
                "        \"\\n\",\n",
                "        \"# 3. Extract val images\\n\",\n",
                "        \"print(\\\"\\\\nüì¶ Extracting validation images...\\\")\\n\",\n",
                "        \"val_tar = os.path.join(download_dir, 'val.tar')\\n\",\n",
                "        \"val_dir = os.path.join(DATASET_PATH, 'val_images')\\n\",\n",
                "        \"temp_val = '/content/temp_val'\\n\",\n",
                "        \"\\n\",\n",
                "        \"if os.path.exists(val_tar):\\n\",\n",
                "        \"    os.makedirs(val_dir, exist_ok=True)\\n\",\n",
                "        \"    \\n\",\n",
                "        \"    if os.path.exists(temp_val):\\n\",\n",
                "        \"        shutil.rmtree(temp_val)\\n\",\n",
                "        \"    \\n\",\n",
                "        \"    safe_extract(val_tar, temp_val)\\n\",\n",
                "        \"    \\n\",\n",
                "        \"    src_dir = os.path.join(temp_val, 'val_set')\\n\",\n",
                "        \"    if not os.path.exists(src_dir):\\n\",\n",
                "        \"        subdirs = os.listdir(temp_val)\\n\",\n",
                "        \"        if subdirs:\\n\",\n",
                "        \"            src_dir = os.path.join(temp_val, subdirs[0])\\n\",\n",
                "        \"    \\n\",\n",
                "        \"    if os.path.exists(src_dir):\\n\",\n",
                "        \"        count = move_files_safe(src_dir, val_dir)\\n\",\n",
                "        \"        shutil.rmtree(temp_val, ignore_errors=True)\\n\",\n",
                "        \"        print(f\\\"‚úÖ Val images: {len(os.listdir(val_dir))} files\\\")\\n\",\n",
                "        \"else:\\n\",\n",
                "        \"    print(f\\\"‚è≠Ô∏è  Val tar not found, checking existing: {len(os.listdir(val_dir)) if os.path.exists(val_dir) else 0} files\\\")\\n\",\n",
                "        \"\\n\",\n",
                "        \"# 4. Extract test images\\n\",\n",
                "        \"print(\\\"\\\\nüì¶ Extracting test images...\\\")\\n\",\n",
                "        \"test_tar = os.path.join(download_dir, 'test.tar')\\n\",\n",
                "        \"test_dir = os.path.join(DATASET_PATH, 'test_images')\\n\",\n",
                "        \"temp_test = '/content/temp_test'\\n\",\n",
                "        \"\\n\",\n",
                "        \"if os.path.exists(test_tar):\\n\",\n",
                "        \"    os.makedirs(test_dir, exist_ok=True)\\n\",\n",
                "        \"    \\n\",\n",
                "        \"    if os.path.exists(temp_test):\\n\",\n",
                "        \"        shutil.rmtree(temp_test)\\n\",\n",
                "        \"    \\n\",\n",
                "        \"    safe_extract(test_tar, temp_test)\\n\",\n",
                "        \"    \\n\",\n",
                "        \"    src_dir = os.path.join(temp_test, 'test_set')\\n\",\n",
                "        \"    if not os.path.exists(src_dir):\\n\",\n",
                "        \"        subdirs = os.listdir(temp_test)\\n\",\n",
                "        \"        if subdirs:\\n\",\n",
                "        \"            src_dir = os.path.join(temp_test, subdirs[0])\\n\",\n",
                "        \"    \\n\",\n",
                "        \"    if os.path.exists(src_dir):\\n\",\n",
                "        \"        count = move_files_safe(src_dir, test_dir)\\n\",\n",
                "        \"        shutil.rmtree(temp_test, ignore_errors=True)\\n\",\n",
                "        \"        print(f\\\"‚úÖ Test images: {len(os.listdir(test_dir))} files\\\")\\n\",\n",
                "        \"else:\\n\",\n",
                "        \"    print(f\\\"‚è≠Ô∏è  Test tar not found, checking existing: {len(os.listdir(test_dir)) if os.path.exists(test_dir) else 0} files\\\")\\n\",\n",
                "        \"\\n\",\n",
                "        \"print(\\\"\\\\n\\\" + \\\"=\\\"*60)\\n\",\n",
                "        \"print(\\\"‚úÖ Ekstraksi selesai!\\\")\"\n",
                "      ]\n",
                "    },\n",
                "    {\n",
                "      \"cell_type\": \"code\",\n",
                "      \"execution_count\": null,\n",
                "      \"metadata\": {},\n",
                "      \"outputs\": [],\n",
                "      \"source\": [\n",
                "        \"# ============================================================\\n\",\n",
                "        \"# STEP 4: Verifikasi Dataset\\n\",\n",
                "        \"# ============================================================\\n\",\n",
                "        \"\\n\",\n",
                "        \"import os\\n\",\n",
                "        \"\\n\",\n",
                "        \"DATASET_PATH = '/content/drive/MyDrive/AlexNet_iFood2019/dataset'\\n\",\n",
                "        \"\\n\",\n",
                "        \"print(\\\"üîç Verifikasi dataset...\\\")\\n\",\n",
                "        \"print(\\\"=\\\"*60)\\n\",\n",
                "        \"\\n\",\n",
                "        \"# Check files\\n\",\n",
                "        \"required_items = {\\n\",\n",
                "        \"    'class_list.txt': 'file',\\n\",\n",
                "        \"    'train_info.csv': 'file',\\n\",\n",
                "        \"    'val_info.csv': 'file',\\n\",\n",
                "        \"    'test_info.csv': 'file',\\n\",\n",
                "        \"    'train_images': 'dir',\\n\",\n",
                "        \"    'val_images': 'dir',\\n\",\n",
                "        \"    'test_images': 'dir'\\n\",\n",
                "        \"}\\n\",\n",
                "        \"\\n\",\n",
                "        \"all_ok = True\\n\",\n",
                "        \"for item, item_type in required_items.items():\\n\",\n",
                "        \"    path = os.path.join(DATASET_PATH, item)\\n\",\n",
                "        \"    \\n\",\n",
                "        \"    if item_type == 'file':\\n\",\n",
                "        \"        exists = os.path.isfile(path)\\n\",\n",
                "        \"    else:\\n\",\n",
                "        \"        exists = os.path.isdir(path)\\n\",\n",
                "        \"    \\n\",\n",
                "        \"    if exists:\\n\",\n",
                "        \"        if item_type == 'dir':\\n\",\n",
                "        \"            count = len(os.listdir(path))\\n\",\n",
                "        \"            print(f\\\"‚úÖ {item}: {count:,} files\\\")\\n\",\n",
                "        \"        else:\\n\",\n",
                "        \"            print(f\\\"‚úÖ {item}\\\")\\n\",\n",
                "        \"    else:\\n\",\n",
                "        \"        print(f\\\"‚ùå {item}: TIDAK DITEMUKAN\\\")\\n\",\n",
                "        \"        all_ok = False\\n\",\n",
                "        \"\\n\",\n",
                "        \"print(\\\"\\\\n\\\" + \\\"=\\\"*60)\\n\",\n",
                "        \"if all_ok:\\n\",\n",
                "        \"    print(\\\"üéâ DATASET SIAP DIGUNAKAN!\\\")\\n\",\n",
                "        \"    print(f\\\"\\\\nLokasi: {DATASET_PATH}\\\")\\n\",\n",
                "        \"else:\\n\",\n",
                "        \"    print(\\\"‚ö†Ô∏è  Ada file yang hilang, coba jalankan ulang dari Step 2\\\")\"\n",
                "      ]\n",
                "    },\n",
                "    {\n",
                "      \"cell_type\": \"code\",\n",
                "      \"execution_count\": null,\n",
                "      \"metadata\": {},\n",
                "      \"outputs\": [],\n",
                "      \"source\": [\n",
                "        \"# ============================================================\\n\",\n",
                "        \"# STEP 5: Cleanup (Hapus file tar untuk hemat storage)\\n\",\n",
                "        \"# ============================================================\\n\",\n",
                "        \"\\n\",\n",
                "        \"import shutil\\n\",\n",
                "        \"import os\\n\",\n",
                "        \"\\n\",\n",
                "        \"download_dir = '/content/downloads'\\n\",\n",
                "        \"\\n\",\n",
                "        \"if os.path.exists(download_dir):\\n\",\n",
                "        \"    size_before = sum(os.path.getsize(os.path.join(download_dir, f)) \\n\",\n",
                "        \"                      for f in os.listdir(download_dir) \\n\",\n",
                "        \"                      if os.path.isfile(os.path.join(download_dir, f)))\\n\",\n",
                "        \"    \\n\",\n",
                "        \"    shutil.rmtree(download_dir)\\n\",\n",
                "        \"    print(f\\\"üóëÔ∏è  Deleted download cache: {size_before / 1e9:.2f} GB freed\\\")\\n\",\n",
                "        \"else:\\n\",\n",
                "        \"    print(\\\"‚úÖ No cache to clean\\\")\\n\",\n",
                "        \"\\n\",\n",
                "        \"# Cleanup temp folders juga\\n\",\n",
                "        \"for temp in ['/content/temp_train', '/content/temp_val', '/content/temp_test']:\\n\",\n",
                "        \"    if os.path.exists(temp):\\n\",\n",
                "        \"        shutil.rmtree(temp, ignore_errors=True)\\n\",\n",
                "        \"\\n\",\n",
                "        \"print(\\\"\\\\n\\\" + \\\"=\\\"*60)\\n\",\n",
                "        \"print(\\\"üéâ SELESAI!\\\")\\n\",\n",
                "        \"print(\\\"=\\\"*60)\\n\",\n",
                "        \"print(\\\"\\\\nDataset sudah tersimpan di Google Drive.\\\")\\n\",\n",
                "        \"print(\\\"Anda bisa menutup notebook ini dan lanjut ke training.\\\")\\n\",\n",
                "        \"print(\\\"\\\\nNotebook selanjutnya:\\\")\\n\",\n",
                "        \"print(\\\"  - train_member1_baseline.ipynb (Member 1)\\\")\\n\",\n",
                "        \"print(\\\"  - train_member2_mod1.ipynb (Member 2)\\\")\\n\",\n",
                "        \"print(\\\"  - train_member3_mod2.ipynb (Member 3)\\\")\\n\",\n",
                "        \"print(\\\"  - train_member4_combined.ipynb (Member 4)\\\")\"\n",
                "      ]\n",
                "    }\n",
                "  ],\n",
                "  \"metadata\": {\n",
                "    \"kernelspec\": {\n",
                "      \"display_name\": \"Python 3\",\n",
                "      \"language\": \"python\",\n",
                "      \"name\": \"python3\"\n",
                "    },\n",
                "    \"accelerator\": \"GPU\"\n",
                "  },\n",
                "  \"nbformat\": 4,\n",
                "  \"nbformat_minor\": 4\n",
                "}"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
