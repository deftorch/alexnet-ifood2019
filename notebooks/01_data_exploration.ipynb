{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# AlexNet iFood2019 - Data Exploration\n",
                "\n",
                "This notebook explores the iFood 2019 dataset.\n",
                "\n",
                "## Contents:\n",
                "1. Setup & Load Data\n",
                "2. Dataset Statistics\n",
                "3. Class Distribution Analysis\n",
                "4. Sample Visualization\n",
                "5. Test Data Loader"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# Cell 1: Quick Setup\n",
                "# ============================================================\n",
                "\n",
                "from google.colab import drive\n",
                "drive.mount('/content/drive')\n",
                "\n",
                "import os\n",
                "import sys\n",
                "\n",
                "PROJECT_PATH = '/content/drive/MyDrive/AlexNet_iFood2019'\n",
                "REPO_PATH = '/content/alexnet-ifood2019'\n",
                "\n",
                "# Clone if needed\n",
                "if not os.path.exists(REPO_PATH):\n",
                "    !git clone https://github.com/deftorch/alexnet-ifood2019.git {REPO_PATH}\n",
                "\n",
                "os.chdir(REPO_PATH)\n",
                "sys.path.insert(0, REPO_PATH)\n",
                "sys.path.insert(0, os.path.join(REPO_PATH, 'src'))\n",
                "\n",
                "print(f\"✓ Setup complete\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# Cell 2: Check/Create Dataset\n",
                "# ============================================================\n",
                "\n",
                "import os\n",
                "\n",
                "DATA_DIR = os.path.join(PROJECT_PATH, 'dataset')\n",
                "\n",
                "# Check if dataset exists\n",
                "required = ['train_images', 'val_images', 'train_info.csv', 'val_info.csv']\n",
                "missing = [f for f in required if not os.path.exists(os.path.join(DATA_DIR, f))]\n",
                "\n",
                "if missing:\n",
                "    print(\"⚠️  Dataset not found. Creating mock data for testing...\")\n",
                "    !python src/create_mock_data.py --output_dir {DATA_DIR} --train_per_class 5 --val_per_class 2\n",
                "else:\n",
                "    print(\"✓ Dataset found!\")\n",
                "    \n",
                "# Create symlink\n",
                "!rm -rf data\n",
                "!ln -s {DATA_DIR} data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# Cell 3: Load and Explore Dataset Statistics\n",
                "# ============================================================\n",
                "\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "\n",
                "# Load annotations\n",
                "train_df = pd.read_csv(os.path.join(DATA_DIR, 'train_info.csv'), header=None, names=['image', 'label'])\n",
                "val_df = pd.read_csv(os.path.join(DATA_DIR, 'val_info.csv'), header=None, names=['image', 'label'])\n",
                "\n",
                "# Load class names\n",
                "class_file = os.path.join(DATA_DIR, 'class_list.txt')\n",
                "if os.path.exists(class_file):\n",
                "    with open(class_file) as f:\n",
                "        classes = [line.strip().split(' ', 1)[1] for line in f.readlines()]\n",
                "else:\n",
                "    classes = [f'class_{i}' for i in range(251)]\n",
                "\n",
                "print(\"Dataset Statistics\")\n",
                "print(\"=\" * 50)\n",
                "print(f\"Total classes: {len(classes)}\")\n",
                "print(f\"Training samples: {len(train_df):,}\")\n",
                "print(f\"Validation samples: {len(val_df):,}\")\n",
                "print(f\"\\nSample class names: {classes[:5]}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# Cell 4: Class Distribution Analysis\n",
                "# ============================================================\n",
                "\n",
                "# Analyze class distribution\n",
                "class_counts = train_df['label'].value_counts().sort_index()\n",
                "\n",
                "print(\"Class Distribution Statistics:\")\n",
                "print(f\"  Mean samples per class: {class_counts.mean():.0f}\")\n",
                "print(f\"  Median: {class_counts.median():.0f}\")\n",
                "print(f\"  Min: {class_counts.min()}\")\n",
                "print(f\"  Max: {class_counts.max()}\")\n",
                "print(f\"  Std: {class_counts.std():.2f}\")\n",
                "if class_counts.min() > 0:\n",
                "    print(f\"  Imbalance ratio: {class_counts.max() / class_counts.min():.2f}x\")\n",
                "\n",
                "# Plot distribution\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Histogram\n",
                "axes[0].hist(class_counts.values, bins=30, edgecolor='black', alpha=0.7)\n",
                "axes[0].set_xlabel('Number of Samples')\n",
                "axes[0].set_ylabel('Number of Classes')\n",
                "axes[0].set_title('Distribution of Samples per Class')\n",
                "axes[0].grid(alpha=0.3)\n",
                "\n",
                "# Sorted distribution\n",
                "sorted_counts = class_counts.sort_values(ascending=False)\n",
                "axes[1].plot(range(len(sorted_counts)), sorted_counts.values, linewidth=2)\n",
                "axes[1].fill_between(range(len(sorted_counts)), sorted_counts.values, alpha=0.3)\n",
                "axes[1].set_xlabel('Class Rank')\n",
                "axes[1].set_ylabel('Number of Samples')\n",
                "axes[1].set_title('Sorted Class Distribution')\n",
                "axes[1].grid(alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(os.path.join(PROJECT_PATH, 'analysis_results', 'class_distribution.png'), dpi=150)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# Cell 5: Visualize Sample Images\n",
                "# ============================================================\n",
                "\n",
                "from PIL import Image\n",
                "import random\n",
                "\n",
                "# Sample images\n",
                "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
                "\n",
                "for i in range(2):\n",
                "    for j in range(5):\n",
                "        idx = random.randint(0, len(train_df) - 1)\n",
                "        img_name = train_df.iloc[idx]['image']\n",
                "        label = train_df.iloc[idx]['label']\n",
                "        \n",
                "        img_path = os.path.join(DATA_DIR, 'train_images', img_name)\n",
                "        \n",
                "        try:\n",
                "            img = Image.open(img_path)\n",
                "            axes[i, j].imshow(img)\n",
                "            class_name = classes[label] if label < len(classes) else f'class_{label}'\n",
                "            axes[i, j].set_title(f\"{class_name[:15]}...\", fontsize=9)\n",
                "        except Exception as e:\n",
                "            axes[i, j].text(0.5, 0.5, 'Error', ha='center', va='center')\n",
                "        \n",
                "        axes[i, j].axis('off')\n",
                "\n",
                "plt.suptitle('Random Training Samples', fontsize=14)\n",
                "plt.tight_layout()\n",
                "plt.savefig(os.path.join(PROJECT_PATH, 'analysis_results', 'sample_images.png'), dpi=150)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# Cell 6: Test Data Loader\n",
                "# ============================================================\n",
                "\n",
                "from src.data_loader import get_dataloaders, get_transforms\n",
                "\n",
                "print(\"Testing data loaders...\")\n",
                "\n",
                "dataloaders = get_dataloaders(\n",
                "    data_dir=DATA_DIR,\n",
                "    batch_size=32,\n",
                "    num_workers=2\n",
                ")\n",
                "\n",
                "for split, loader in dataloaders.items():\n",
                "    print(f\"\\n{split.upper()}:\")\n",
                "    print(f\"  Batches: {len(loader)}\")\n",
                "    print(f\"  Samples: {len(loader.dataset)}\")\n",
                "\n",
                "# Test one batch\n",
                "images, labels = next(iter(dataloaders['train']))\n",
                "print(f\"\\nBatch shapes:\")\n",
                "print(f\"  Images: {images.shape}\")\n",
                "print(f\"  Labels: {labels.shape}\")\n",
                "print(f\"  Image range: [{images.min():.2f}, {images.max():.2f}]\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# Cell 7: Visualize Augmented Images\n",
                "# ============================================================\n",
                "\n",
                "from src.data_loader import get_transforms\n",
                "import torch\n",
                "\n",
                "# Get a sample image\n",
                "sample_idx = 0\n",
                "img_name = train_df.iloc[sample_idx]['image']\n",
                "img_path = os.path.join(DATA_DIR, 'train_images', img_name)\n",
                "original_img = Image.open(img_path)\n",
                "\n",
                "# Get train transforms\n",
                "train_transform = get_transforms('train')\n",
                "\n",
                "# Show original and 5 augmented versions\n",
                "fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
                "\n",
                "# Original\n",
                "axes[0, 0].imshow(original_img)\n",
                "axes[0, 0].set_title('Original')\n",
                "axes[0, 0].axis('off')\n",
                "\n",
                "# Augmented versions\n",
                "for i in range(5):\n",
                "    row = (i + 1) // 3\n",
                "    col = (i + 1) % 3\n",
                "    \n",
                "    augmented = train_transform(original_img)\n",
                "    \n",
                "    # Denormalize for visualization\n",
                "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
                "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
                "    augmented_denorm = augmented * std + mean\n",
                "    augmented_denorm = torch.clamp(augmented_denorm, 0, 1)\n",
                "    \n",
                "    axes[row, col].imshow(augmented_denorm.permute(1, 2, 0).numpy())\n",
                "    axes[row, col].set_title(f'Augmented {i+1}')\n",
                "    axes[row, col].axis('off')\n",
                "\n",
                "plt.suptitle('Data Augmentation Examples', fontsize=14)\n",
                "plt.tight_layout()\n",
                "plt.savefig(os.path.join(PROJECT_PATH, 'analysis_results', 'augmentation_examples.png'), dpi=150)\n",
                "plt.show()\n",
                "\n",
                "print(\"\\n✓ Data exploration complete!\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "accelerator": "GPU"
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
