{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# AlexNet iFood2019 - Analysis and Visualization\n",
                "\n",
                "This notebook provides comprehensive analysis of trained models:\n",
                "1. Load all results\n",
                "2. Compare training curves\n",
                "3. Analyze metrics\n",
                "4. Generate visualizations\n",
                "5. Create summary report"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# Cell 1: Setup\n",
                "# ============================================================\n",
                "\n",
                "from google.colab import drive\n",
                "drive.mount('/content/drive')\n",
                "\n",
                "import os\n",
                "import sys\n",
                "import json\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "\n",
                "PROJECT_PATH = '/content/drive/MyDrive/AlexNet_iFood2019'\n",
                "CHECKPOINT_DIR = os.path.join(PROJECT_PATH, 'checkpoints')\n",
                "EVAL_DIR = os.path.join(PROJECT_PATH, 'evaluation_results')\n",
                "ANALYSIS_DIR = os.path.join(PROJECT_PATH, 'analysis_results')\n",
                "\n",
                "MODELS = ['alexnet_baseline', 'alexnet_mod1', 'alexnet_mod2', 'alexnet_combined']\n",
                "\n",
                "print(\"‚úì Setup complete\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# Cell 2: Load Training Histories\n",
                "# ============================================================\n",
                "\n",
                "histories = {}\n",
                "\n",
                "for model in MODELS:\n",
                "    history_file = os.path.join(CHECKPOINT_DIR, f'{model}_history.json')\n",
                "    if os.path.exists(history_file):\n",
                "        with open(history_file) as f:\n",
                "            histories[model] = json.load(f)\n",
                "        print(f\"‚úì Loaded: {model}\")\n",
                "    else:\n",
                "        print(f\"‚ö†Ô∏è  Not found: {model}\")\n",
                "\n",
                "print(f\"\\nLoaded {len(histories)} training histories\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# Cell 3: Load Evaluation Metrics\n",
                "# ============================================================\n",
                "\n",
                "metrics = {}\n",
                "\n",
                "for model in MODELS:\n",
                "    metrics_file = os.path.join(EVAL_DIR, f'{model}_val_metrics.json')\n",
                "    if os.path.exists(metrics_file):\n",
                "        with open(metrics_file) as f:\n",
                "            metrics[model] = json.load(f)\n",
                "        print(f\"‚úì Loaded: {model}\")\n",
                "    else:\n",
                "        print(f\"‚ö†Ô∏è  Not found: {model}\")\n",
                "\n",
                "print(f\"\\nLoaded {len(metrics)} evaluation results\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# Cell 4: Training Curves Comparison\n",
                "# ============================================================\n",
                "\n",
                "if histories:\n",
                "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
                "    colors = plt.cm.tab10(np.linspace(0, 1, len(histories)))\n",
                "    \n",
                "    # Train Loss\n",
                "    ax = axes[0, 0]\n",
                "    for (model, hist), color in zip(histories.items(), colors):\n",
                "        epochs = range(1, len(hist['train_loss']) + 1)\n",
                "        ax.plot(epochs, hist['train_loss'], color=color, label=model, linewidth=2)\n",
                "    ax.set_xlabel('Epoch')\n",
                "    ax.set_ylabel('Loss')\n",
                "    ax.set_title('Training Loss')\n",
                "    ax.legend()\n",
                "    ax.grid(alpha=0.3)\n",
                "    \n",
                "    # Val Loss\n",
                "    ax = axes[0, 1]\n",
                "    for (model, hist), color in zip(histories.items(), colors):\n",
                "        epochs = range(1, len(hist['val_loss']) + 1)\n",
                "        ax.plot(epochs, hist['val_loss'], color=color, label=model, linewidth=2)\n",
                "    ax.set_xlabel('Epoch')\n",
                "    ax.set_ylabel('Loss')\n",
                "    ax.set_title('Validation Loss')\n",
                "    ax.legend()\n",
                "    ax.grid(alpha=0.3)\n",
                "    \n",
                "    # Train Acc\n",
                "    ax = axes[1, 0]\n",
                "    for (model, hist), color in zip(histories.items(), colors):\n",
                "        epochs = range(1, len(hist['train_acc']) + 1)\n",
                "        ax.plot(epochs, hist['train_acc'], color=color, label=model, linewidth=2)\n",
                "    ax.set_xlabel('Epoch')\n",
                "    ax.set_ylabel('Accuracy')\n",
                "    ax.set_title('Training Accuracy')\n",
                "    ax.legend()\n",
                "    ax.grid(alpha=0.3)\n",
                "    \n",
                "    # Val Acc\n",
                "    ax = axes[1, 1]\n",
                "    for (model, hist), color in zip(histories.items(), colors):\n",
                "        epochs = range(1, len(hist['val_acc']) + 1)\n",
                "        ax.plot(epochs, hist['val_acc'], color=color, label=model, linewidth=2)\n",
                "    ax.set_xlabel('Epoch')\n",
                "    ax.set_ylabel('Accuracy')\n",
                "    ax.set_title('Validation Accuracy')\n",
                "    ax.legend()\n",
                "    ax.grid(alpha=0.3)\n",
                "    \n",
                "    plt.suptitle('Training Curves Comparison', fontsize=14)\n",
                "    plt.tight_layout()\n",
                "    plt.savefig(os.path.join(ANALYSIS_DIR, 'training_curves_detailed.png'), dpi=200)\n",
                "    plt.show()\n",
                "else:\n",
                "    print(\"No training histories available\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# Cell 5: Metrics Bar Chart\n",
                "# ============================================================\n",
                "\n",
                "if metrics:\n",
                "    metric_names = ['accuracy', 'top5_accuracy', 'precision', 'recall', 'macro_f1']\n",
                "    \n",
                "    fig, ax = plt.subplots(figsize=(12, 6))\n",
                "    \n",
                "    x = np.arange(len(metrics))\n",
                "    width = 0.15\n",
                "    \n",
                "    for i, metric in enumerate(metric_names):\n",
                "        values = [metrics[m].get(metric, 0) for m in metrics]\n",
                "        bars = ax.bar(x + i * width, values, width, label=metric.replace('_', ' ').title())\n",
                "        \n",
                "        # Add value labels\n",
                "        for bar, val in zip(bars, values):\n",
                "            ax.annotate(f'{val:.3f}',\n",
                "                       xy=(bar.get_x() + bar.get_width() / 2, bar.get_height()),\n",
                "                       xytext=(0, 3), textcoords='offset points',\n",
                "                       ha='center', va='bottom', fontsize=8, rotation=45)\n",
                "    \n",
                "    ax.set_xlabel('Model')\n",
                "    ax.set_ylabel('Score')\n",
                "    ax.set_title('Model Performance Comparison')\n",
                "    ax.set_xticks(x + width * 2)\n",
                "    ax.set_xticklabels([m.replace('alexnet_', '') for m in metrics])\n",
                "    ax.legend(loc='upper right')\n",
                "    ax.set_ylim(0, 1.15)\n",
                "    ax.grid(alpha=0.3, axis='y')\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.savefig(os.path.join(ANALYSIS_DIR, 'metrics_comparison_detailed.png'), dpi=200)\n",
                "    plt.show()\n",
                "else:\n",
                "    print(\"No metrics available\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# Cell 6: Summary Table\n",
                "# ============================================================\n",
                "\n",
                "data = []\n",
                "\n",
                "for model in MODELS:\n",
                "    row = {'Model': model.replace('alexnet_', '').upper()}\n",
                "    \n",
                "    if model in histories:\n",
                "        hist = histories[model]\n",
                "        row['Train Loss (final)'] = f\"{hist['train_loss'][-1]:.4f}\"\n",
                "        row['Best Val Acc'] = f\"{max(hist['val_acc']):.4f}\"\n",
                "        row['Best Epoch'] = hist['val_acc'].index(max(hist['val_acc'])) + 1\n",
                "    \n",
                "    if model in metrics:\n",
                "        m = metrics[model]\n",
                "        row['Test Accuracy'] = f\"{m.get('accuracy', 0):.4f}\"\n",
                "        row['Top-5 Accuracy'] = f\"{m.get('top5_accuracy', 0):.4f}\"\n",
                "        row['Macro F1'] = f\"{m.get('macro_f1', 0):.4f}\"\n",
                "    \n",
                "    data.append(row)\n",
                "\n",
                "df = pd.DataFrame(data)\n",
                "print(\"\\n\" + \"=\" * 90)\n",
                "print(\"MODEL COMPARISON SUMMARY\")\n",
                "print(\"=\" * 90)\n",
                "print(df.to_string(index=False))\n",
                "print(\"=\" * 90)\n",
                "\n",
                "# Save to CSV\n",
                "df.to_csv(os.path.join(ANALYSIS_DIR, 'model_comparison.csv'), index=False)\n",
                "print(f\"\\n‚úì Table saved to: {ANALYSIS_DIR}/model_comparison.csv\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# Cell 7: Best Model Analysis\n",
                "# ============================================================\n",
                "\n",
                "if metrics:\n",
                "    # Find best model by accuracy\n",
                "    best_model = max(metrics, key=lambda m: metrics[m].get('accuracy', 0))\n",
                "    best_acc = metrics[best_model].get('accuracy', 0)\n",
                "    \n",
                "    print(\"üèÜ BEST MODEL ANALYSIS\")\n",
                "    print(\"=\" * 50)\n",
                "    print(f\"Model: {best_model}\")\n",
                "    print(f\"Accuracy: {best_acc:.4f} ({best_acc*100:.2f}%)\")\n",
                "    print(f\"Top-5 Accuracy: {metrics[best_model].get('top5_accuracy', 0):.4f}\")\n",
                "    print(f\"Macro F1: {metrics[best_model].get('macro_f1', 0):.4f}\")\n",
                "    print(f\"Weighted F1: {metrics[best_model].get('weighted_f1', 0):.4f}\")\n",
                "    print(\"=\" * 50)\n",
                "    \n",
                "    # Improvement over baseline\n",
                "    if 'alexnet_baseline' in metrics:\n",
                "        baseline_acc = metrics['alexnet_baseline'].get('accuracy', 0)\n",
                "        improvement = best_acc - baseline_acc\n",
                "        print(f\"\\nImprovement over baseline: {improvement:.4f} ({improvement*100:.2f}%)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# Cell 8: Generate Report Markdown\n",
                "# ============================================================\n",
                "\n",
                "report = f\"\"\"# AlexNet iFood2019 - Results Report\n",
                "\n",
                "## Model Comparison\n",
                "\n",
                "{df.to_markdown(index=False)}\n",
                "\n",
                "## Key Findings\n",
                "\n",
                "1. **Best Model**: {best_model}\n",
                "2. **Best Accuracy**: {best_acc:.4f} ({best_acc*100:.2f}%)\n",
                "3. **Top-5 Accuracy**: {metrics[best_model].get('top5_accuracy', 0):.4f}\n",
                "\n",
                "## Training Observations\n",
                "\n",
                "- All models trained for {NUM_EPOCHS if 'NUM_EPOCHS' in dir() else 50} epochs\n",
                "- Batch size: 128\n",
                "- Learning rate: 0.01 with step decay\n",
                "\n",
                "## Plots\n",
                "\n",
                "See generated images in `analysis_results/` folder.\n",
                "\"\"\"\n",
                "\n",
                "report_path = os.path.join(ANALYSIS_DIR, 'results_report.md')\n",
                "with open(report_path, 'w') as f:\n",
                "    f.write(report)\n",
                "\n",
                "print(f\"‚úì Report saved to: {report_path}\")\n",
                "print(\"\\n\" + \"=\" * 50)\n",
                "print(\"üìä ANALYSIS COMPLETE!\")\n",
                "print(\"=\" * 50)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "accelerator": "GPU"
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
